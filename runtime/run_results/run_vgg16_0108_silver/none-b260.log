WARNING:tensorflow:From /home/zongzan/.conda/envs/STR36/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
WARNING:tensorflow:
The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
  * https://github.com/tensorflow/io (for I/O related ops)
If you depend on functionality not listed there, please file an issue.

WARNING:tensorflow:From /home/zongzan/.conda/envs/STR36/lib/python3.6/site-packages/tensorflow_core/python/ops/stropt/lms.py:970: The name tf.train.SessionRunHook is deprecated. Please use tf.estimator.SessionRunHook instead.

2022-01-08 18:43:12.731175: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.68GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2022-01-08 18:43:12.731270: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.68GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
1 Physical GPUs, 1 Logical GPUs
Parsing arguments: [('batch_size', 260), ('epochs', 1), ('model_name', 'VGG16'), ('strategy', 'none'), ('verbose', False)]
Cannot find config for batch_size=260, use 250 instead
Using original tensorflow without memory optimization
Shape of x_train: (40000, 224, 224, 3), shape of x_test: (10000, 224, 224, 3)
[STR DEBUG] Parsing STR configuration: [{'strategy': 'none', 'verbose': 'false'}]
[STR DEBUG] Solution type none doesn't has corresponding optimizations, run normal training
  1/154 [..............................] - ETA: 1:12:22 - loss: 2.3241 - acc: 0.0654  2/154 [..............................] - ETA: 38:27 - loss: 15813806.1620 - acc: 0.08272022-01-08 18:43:21.769238: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.51GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2022-01-08 18:43:21.770222: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.51GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
  3/154 [..............................] - ETA: 27:13 - loss: 10542554.3183 - acc: 0.08332022-01-08 18:43:24.116753: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.51GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
  4/154 [..............................] - ETA: 21:44 - loss: 7907292.7203 - acc: 0.0856   5/154 [..............................] - ETA: 18:20 - loss: 6326358.7918 - acc: 0.0846  6/154 [>.............................] - ETA: 16:03 - loss: 5271973.3111 - acc: 0.0878  7/154 [>.............................] - ETA: 14:25 - loss: 4518834.6128 - acc: 0.09342022-01-08 18:43:32.736940: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.51GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
  8/154 [>.............................] - ETA: 13:14 - loss: 3953982.4143 - acc: 0.09762022-01-08 18:43:35.120841: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.51GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
  9/154 [>.............................] - ETA: 12:21 - loss: 3514651.3007 - acc: 0.09622022-01-08 18:43:37.535485: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.51GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
 10/154 [>.............................] - ETA: 11:37 - loss: 3163186.4222 - acc: 0.0938 11/154 [=>............................] - ETA: 11:01 - loss: 2875624.2295 - acc: 0.0944 12/154 [=>............................] - ETA: 10:31 - loss: 2635989.0686 - acc: 0.0933 13/154 [=>............................] - ETA: 10:06 - loss: 2433220.8559 - acc: 0.0911 14/154 [=>............................] - ETA: 9:48 - loss: 2259419.5308 - acc: 0.0923 2022-01-08 18:43:50.660062: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.51GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
 15/154 [=>............................] - ETA: 9:34 - loss: 2108791.7153 - acc: 0.0951 16/154 [==>...........................] - ETA: 9:20 - loss: 1976992.3769 - acc: 0.09662022-01-08 18:43:56.671600: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.51GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
 17/154 [==>...........................] - ETA: 9:08 - loss: 1860698.8432 - acc: 0.0957 18/154 [==>...........................] - ETA: 8:55 - loss: 1757326.8132 - acc: 0.0962 19/154 [==>...........................] - ETA: 8:44 - loss: 1664836.0490 - acc: 0.0978 20/154 [==>...........................] - ETA: 8:34 - loss: 1581594.3614 - acc: 0.0985 21/154 [===>..........................] - ETA: 8:26 - loss: 1506280.4532 - acc: 0.1002 22/154 [===>..........................] - ETA: 8:17 - loss: 1437813.2658 - acc: 0.1012 23/154 [===>..........................] - ETA: 8:10 - loss: 1375299.7464 - acc: 0.1015 24/154 [===>..........................] - ETA: 8:02 - loss: 1317995.6862 - acc: 0.1016 25/154 [===>..........................] - ETA: 7:55 - loss: 1265275.9508 - acc: 0.1018 26/154 [====>.........................] - ETA: 7:48 - loss: 1216611.5796 - acc: 0.1016 27/154 [====>.........................] - ETA: 7:41 - loss: 1171551.9764 - acc: 0.1011 28/154 [====>.........................] - ETA: 7:34 - loss: 1129710.9166 - acc: 0.1007 29/154 [====>.........................] - ETA: 7:28 - loss: 1090755.4472 - acc: 0.1001 30/154 [====>.........................] - ETA: 7:22 - loss: 1054397.0089 - acc: 0.1008 31/154 [=====>........................] - ETA: 7:17 - loss: 1020384.2755 - acc: 0.1009 32/154 [=====>........................] - ETA: 7:12 - loss: 988497.3437 - acc: 0.1004  33/154 [=====>........................] - ETA: 7:07 - loss: 958542.9483 - acc: 0.1012 34/154 [=====>........................] - ETA: 7:02 - loss: 930350.5763 - acc: 0.1016 35/154 [=====>........................] - ETA: 6:57 - loss: 903769.1976 - acc: 0.1009 36/154 [======>.......................] - ETA: 6:52 - loss: 878664.5616 - acc: 0.1018 37/154 [======>.......................] - ETA: 6:47 - loss: 854916.9327 - acc: 0.1023 38/154 [======>.......................] - ETA: 6:42 - loss: 832419.1788 - acc: 0.1021 39/154 [======>.......................] - ETA: 6:37 - loss: 811075.1553 - acc: 0.1023 40/154 [======>.......................] - ETA: 6:33 - loss: 790798.3548 - acc: 0.1028 41/154 [======>.......................] - ETA: 6:28 - loss: 771510.6461 - acc: 0.1032 42/154 [=======>......................] - ETA: 6:25 - loss: 753141.3994 - acc: 0.1041 43/154 [=======>......................] - ETA: 6:20 - loss: 735626.5360 - acc: 0.1047 44/154 [=======>......................] - ETA: 6:15 - loss: 718907.8044 - acc: 0.1038 45/154 [=======>......................] - ETA: 6:10 - loss: 702932.1259 - acc: 0.1038 46/154 [=======>......................] - ETA: 6:06 - loss: 687651.0414 - acc: 0.1062 47/154 [========>.....................] - ETA: 6:01 - loss: 673020.2913 - acc: 0.1056 48/154 [========>.....................] - ETA: 5:56 - loss: 658999.0884 - acc: 0.1056 49/154 [========>.....................] - ETA: 5:52 - loss: 645550.1745 - acc: 0.1046 50/154 [========>.....................] - ETA: 5:48 - loss: 632639.2171 - acc: 0.1050 51/154 [========>.....................] - ETA: 5:43 - loss: 620234.5718 - acc: 0.1046 52/154 [=========>....................] - ETA: 5:40 - loss: 608307.0280 - acc: 0.1041 53/154 [=========>....................] - ETA: 5:36 - loss: 596829.5807 - acc: 0.1036 54/154 [=========>....................] - ETA: 5:32 - loss: 585777.2237 - acc: 0.1032 55/154 [=========>....................] - ETA: 5:28 - loss: 575126.7706 - acc: 0.1031 56/154 [=========>....................] - ETA: 5:24 - loss: 564856.6907 - acc: 0.1037 57/154 [==========>...................] - ETA: 5:20 - loss: 554946.9690 - acc: 0.1041 58/154 [==========>...................] - ETA: 5:17 - loss: 545378.9575 - acc: 0.1038 59/154 [==========>...................] - ETA: 5:13 - loss: 536135.2854 - acc: 0.1035 60/154 [==========>...................] - ETA: 5:10 - loss: 527199.7357 - acc: 0.1030 61/154 [==========>...................] - ETA: 5:06 - loss: 518557.1548 - acc: 0.1030 62/154 [===========>..................] - ETA: 5:03 - loss: 510193.3669 - acc: 0.1027 63/154 [===========>..................] - ETA: 4:59 - loss: 502095.0963 - acc: 0.1029 64/154 [===========>..................] - ETA: 4:56 - loss: 494249.8964 - acc: 0.1027 65/154 [===========>..................] - ETA: 4:52 - loss: 486646.0873 - acc: 0.1027 66/154 [===========>..................] - ETA: 4:49 - loss: 479272.6966 - acc: 0.1029 67/154 [============>.................] - ETA: 4:46 - loss: 472119.4071 - acc: 0.1029 68/154 [============>.................] - ETA: 4:42 - loss: 465176.5085 - acc: 0.1028 69/154 [============>.................] - ETA: 4:39 - loss: 458434.8534 - acc: 0.1031 70/154 [============>.................] - ETA: 4:35 - loss: 451885.8169 - acc: 0.1035 71/154 [============>.................] - ETA: 4:32 - loss: 445521.2604 - acc: 0.10382022-01-08 18:46:54.699553: W tensorflow/core/common_runtime/bfc_allocator.cc:419] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.11GiB (rounded to 3339714560).  Current allocation summary follows.
2022-01-08 18:46:54.700710: W tensorflow/core/common_runtime/bfc_allocator.cc:424] ____________*______*****************************_______**************_________________***_**___*****
2022-01-08 18:46:54.700787: W tensorflow/core/framework/op_kernel.cc:1651] OP_REQUIRES failed at conv_grad_input_ops.cc:1063 : Resource exhausted: OOM when allocating tensor with shape[260,64,224,224] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
Traceback (most recent call last):
  File "../run_training.py", line 83, in <module>
    verbose=1)
  File "/home/zongzan/.conda/envs/STR36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py", line 728, in fit
    use_multiprocessing=use_multiprocessing)
  File "/home/zongzan/.conda/envs/STR36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_generator.py", line 603, in fit
    steps_name='steps_per_epoch')
  File "/home/zongzan/.conda/envs/STR36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_generator.py", line 265, in model_iteration
    batch_outs = batch_function(*batch_data)
  File "/home/zongzan/.conda/envs/STR36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py", line 1021, in train_on_batch
    outputs = self.train_function(ins)  # pylint: disable=not-callable
  File "/home/zongzan/.conda/envs/STR36/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py", line 3476, in __call__
    run_metadata=self.run_metadata)
  File "/home/zongzan/.conda/envs/STR36/lib/python3.6/site-packages/tensorflow_core/python/client/session.py", line 1472, in __call__
    run_metadata_ptr)
tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[260,64,224,224] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
	 [[{{node training/RMSprop/gradients/gradients/block1_conv2/Conv2D_grad/Conv2DBackpropInput}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.


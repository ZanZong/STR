WARNING:tensorflow:From /home/zongzan/.conda/envs/STR36/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
WARNING:tensorflow:
The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
  * https://github.com/tensorflow/io (for I/O related ops)
If you depend on functionality not listed there, please file an issue.

WARNING:tensorflow:From /home/zongzan/.conda/envs/STR36/lib/python3.6/site-packages/tensorflow_core/python/ops/stropt/lms.py:970: The name tf.train.SessionRunHook is deprecated. Please use tf.estimator.SessionRunHook instead.

WARNING:tensorflow:From /home/zongzan/.conda/envs/STR36/lib/python3.6/site-packages/tensorflow_core/python/ops/stropt/lms.py:928: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.

WARNING:tensorflow:From /home/zongzan/.conda/envs/STR36/lib/python3.6/site-packages/tensorflow_core/contrib/graph_editor/select.py:554: get_backward_walk_ops (from tensorflow.contrib.graph_editor.select) is deprecated and will be removed after 2019-06-06.
Instructions for updating:
Please use tensorflow.python.ops.op_selector.get_backward_walk_ops.
1 Physical GPUs, 1 Logical GPUs
Parsing arguments: [('batch_size', 270), ('epochs', 1), ('model_name', 'VGG16'), ('strategy', 'str'), ('verbose', False)]
Cannot find config for batch_size=270, use 250 instead
Shape of x_train: (40000, 224, 224, 3), shape of x_test: (10000, 224, 224, 3)
[STR DEBUG] Parsing STR configuration: [{'strategy': 'hybrid', 'verbose': 'false', 'tags': '/home/zongzan/dist_dnn_training/STR/optimizer/logs/VGG16_250/layer_names'}, {'r': '/home/zongzan/dist_dnn_training/STR/optimizer/logs/VGG16_250/R', 'p': '/home/zongzan/dist_dnn_training/STR/optimizer/logs/VGG16_250/P', 'q': '/home/zongzan/dist_dnn_training/STR/optimizer/logs/VGG16_250/PrunedQ'}]
[STR DEBUG] Processing layer 0's swapping, swap out at [1], swap in at [43]
[STR DEBUG] Find swapout ops: name: "input_1"
op: "Placeholder"
attr {
  key: "dtype"
  value {
    type: DT_FLOAT
  }
}
attr {
  key: "shape"
  value {
    shape {
      dim {
        size: -1
      }
      dim {
        size: 224
      }
      dim {
        size: 224
      }
      dim {
        size: 3
      }
    }
  }
}
, 
	choose swap tensor Tensor("input_1:0", shape=(?, 224, 224, 3), dtype=float32), 
	finish at the end of ops: [<tf.Operation 'block1_conv1/Relu' type=Relu>]
[STR DEBUG] Add swap-in op at name: "training/RMSprop/gradients/gradients/block1_conv1/Relu_grad/ReluGrad"
op: "ReluGrad"
input: "training/RMSprop/gradients/gradients/block1_conv2/Conv2D_grad/Conv2DBackpropInput"
input: "block1_conv1/Relu"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
 for consumer [<tf.Operation 'training/RMSprop/gradients/gradients/block1_conv1/Conv2D_grad/Conv2DBackpropFilter' type=Conv2DBackpropFilter>]
[STR DEBUG] Processing layer 1's swapping, swap out at [3], swap in at [8, 41]
[STR DEBUG] Find swapout ops: name: "block1_conv1/Relu"
op: "Relu"
input: "block1_conv1/BiasAdd"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
, 
	choose swap tensor Tensor("block1_conv1/Relu:0", shape=(?, 224, 224, 64), dtype=float32), 
	finish at the end of ops: [<tf.Operation 'block1_pool/MaxPool' type=MaxPool>]
[STR DEBUG] Add swap-in op at name: "block3_conv2/Relu"
op: "Relu"
input: "block3_conv2/BiasAdd"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
 for consumer [<tf.Operation 'training/RMSprop/gradients/gradients/block1_conv1/Relu_grad/ReluGrad' type=ReluGrad>, <tf.Operation 'training/RMSprop/gradients/gradients/block1_conv2/Conv2D_grad/Conv2DBackpropFilter' type=Conv2DBackpropFilter>]
[STR DEBUG] Add swap-in op at name: "training/RMSprop/gradients/gradients/block1_pool/MaxPool_grad/MaxPoolGrad"
op: "MaxPoolGrad"
input: "block1_conv2/Relu"
input: "block1_pool/MaxPool"
input: "training/RMSprop/gradients/gradients/block2_conv1/Conv2D_grad/Conv2DBackpropInput"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
attr {
  key: "data_format"
  value {
    s: "NHWC"
  }
}
attr {
  key: "ksize"
  value {
    list {
      i: 1
      i: 2
      i: 2
      i: 1
    }
  }
}
attr {
  key: "padding"
  value {
    s: "VALID"
  }
}
attr {
  key: "strides"
  value {
    list {
      i: 1
      i: 2
      i: 2
      i: 1
    }
  }
}
 for consumer [<tf.Operation 'training/RMSprop/gradients/gradients/block1_conv1/Relu_grad/ReluGrad' type=ReluGrad>, <tf.Operation 'training/RMSprop/gradients/gradients/block1_conv2/Conv2D_grad/Conv2DBackpropFilter' type=Conv2DBackpropFilter>]
[STR DEBUG] Processing layer 3's swapping, swap out at [4], swap in at [40]
[STR DEBUG] Find swapout ops: name: "block1_pool/MaxPool"
op: "MaxPool"
input: "block1_conv2/Relu"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
attr {
  key: "data_format"
  value {
    s: "NHWC"
  }
}
attr {
  key: "ksize"
  value {
    list {
      i: 1
      i: 2
      i: 2
      i: 1
    }
  }
}
attr {
  key: "padding"
  value {
    s: "VALID"
  }
}
attr {
  key: "strides"
  value {
    list {
      i: 1
      i: 2
      i: 2
      i: 1
    }
  }
}
, 
	choose swap tensor Tensor("block1_pool/MaxPool:0", shape=(?, 112, 112, 64), dtype=float32), 
	finish at the end of ops: [<tf.Operation 'block2_conv1/Relu' type=Relu>]
[STR DEBUG] Add swap-in op at name: "training/RMSprop/gradients/gradients/block2_conv1/Relu_grad/ReluGrad"
op: "ReluGrad"
input: "training/RMSprop/gradients/gradients/block2_conv2/Conv2D_grad/Conv2DBackpropInput"
input: "block2_conv1/Relu"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
 for consumer [<tf.Operation 'training/RMSprop/gradients/gradients/block1_pool/MaxPool_grad/MaxPoolGrad' type=MaxPoolGrad>, <tf.Operation 'training/RMSprop/gradients/gradients/block2_conv1/Conv2D_grad/Conv2DBackpropFilter' type=Conv2DBackpropFilter>]
[STR DEBUG] Processing layer 4's swapping, swap out at [6], swap in at [11, 38]
[STR DEBUG] Find swapout ops: name: "block2_conv1/Relu"
op: "Relu"
input: "block2_conv1/BiasAdd"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
, 
	choose swap tensor Tensor("block2_conv1/Relu:0", shape=(?, 112, 112, 128), dtype=float32), 
	finish at the end of ops: [<tf.Operation 'block2_pool/MaxPool' type=MaxPool>]
[STR DEBUG] Add swap-in op at name: "block4_conv1/Relu"
op: "Relu"
input: "block4_conv1/BiasAdd"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
 for consumer [<tf.Operation 'training/RMSprop/gradients/gradients/block2_conv1/Relu_grad/ReluGrad' type=ReluGrad>, <tf.Operation 'training/RMSprop/gradients/gradients/block2_conv2/Conv2D_grad/Conv2DBackpropFilter' type=Conv2DBackpropFilter>]
[STR DEBUG] Add swap-in op at name: "training/RMSprop/gradients/gradients/block2_pool/MaxPool_grad/MaxPoolGrad"
op: "MaxPoolGrad"
input: "block2_conv2/Relu"
input: "block2_pool/MaxPool"
input: "training/RMSprop/gradients/gradients/block3_conv1/Conv2D_grad/Conv2DBackpropInput"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
attr {
  key: "data_format"
  value {
    s: "NHWC"
  }
}
attr {
  key: "ksize"
  value {
    list {
      i: 1
      i: 2
      i: 2
      i: 1
    }
  }
}
attr {
  key: "padding"
  value {
    s: "VALID"
  }
}
attr {
  key: "strides"
  value {
    list {
      i: 1
      i: 2
      i: 2
      i: 1
    }
  }
}
 for consumer [<tf.Operation 'training/RMSprop/gradients/gradients/block2_conv1/Relu_grad/ReluGrad' type=ReluGrad>, <tf.Operation 'training/RMSprop/gradients/gradients/block2_conv2/Conv2D_grad/Conv2DBackpropFilter' type=Conv2DBackpropFilter>]
[STR DEBUG] Processing layer 6's swapping, swap out at [7], swap in at [37]
[STR DEBUG] Find swapout ops: name: "block2_pool/MaxPool"
op: "MaxPool"
input: "block2_conv2/Relu"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
attr {
  key: "data_format"
  value {
    s: "NHWC"
  }
}
attr {
  key: "ksize"
  value {
    list {
      i: 1
      i: 2
      i: 2
      i: 1
    }
  }
}
attr {
  key: "padding"
  value {
    s: "VALID"
  }
}
attr {
  key: "strides"
  value {
    list {
      i: 1
      i: 2
      i: 2
      i: 1
    }
  }
}
, 
	choose swap tensor Tensor("block2_pool/MaxPool:0", shape=(?, 56, 56, 128), dtype=float32), 
	finish at the end of ops: [<tf.Operation 'block3_conv1/Relu' type=Relu>]
[STR DEBUG] Add swap-in op at name: "training/RMSprop/gradients/gradients/block3_conv1/Relu_grad/ReluGrad"
op: "ReluGrad"
input: "training/RMSprop/gradients/gradients/block3_conv2/Conv2D_grad/Conv2DBackpropInput"
input: "block3_conv1/Relu"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
 for consumer [<tf.Operation 'training/RMSprop/gradients/gradients/block2_pool/MaxPool_grad/MaxPoolGrad' type=MaxPoolGrad>, <tf.Operation 'training/RMSprop/gradients/gradients/block3_conv1/Conv2D_grad/Conv2DBackpropFilter' type=Conv2DBackpropFilter>]
[STR DEBUG] Processing layer 7's swapping, swap out at [8], swap in at [36]
[STR DEBUG] Find swapout ops: name: "block3_conv1/Relu"
op: "Relu"
input: "block3_conv1/BiasAdd"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
, 
	choose swap tensor Tensor("block3_conv1/Relu:0", shape=(?, 56, 56, 256), dtype=float32), 
	finish at the end of ops: [<tf.Operation 'block3_conv2/Relu' type=Relu>]
[STR DEBUG] Add swap-in op at name: "training/RMSprop/gradients/gradients/block3_conv2/Relu_grad/ReluGrad"
op: "ReluGrad"
input: "training/RMSprop/gradients/gradients/block3_conv3/Conv2D_grad/Conv2DBackpropInput"
input: "block3_conv2/Relu"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
 for consumer [<tf.Operation 'training/RMSprop/gradients/gradients/block3_conv1/Relu_grad/ReluGrad' type=ReluGrad>, <tf.Operation 'training/RMSprop/gradients/gradients/block3_conv2/Conv2D_grad/Conv2DBackpropFilter' type=Conv2DBackpropFilter>]
[STR DEBUG] Processing layer 8's swapping, swap out at [10], swap in at [14, 34]
[STR DEBUG] Find swapout ops: name: "block3_conv2/Relu"
op: "Relu"
input: "block3_conv2/BiasAdd"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
, 
	choose swap tensor Tensor("block3_conv2/Relu:0", shape=(?, 56, 56, 256), dtype=float32), 
	finish at the end of ops: [<tf.Operation 'block3_pool/MaxPool' type=MaxPool>]
[STR DEBUG] Add swap-in op at name: "block4_pool/MaxPool"
op: "MaxPool"
input: "block4_conv3/Relu"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
attr {
  key: "data_format"
  value {
    s: "NHWC"
  }
}
attr {
  key: "ksize"
  value {
    list {
      i: 1
      i: 2
      i: 2
      i: 1
    }
  }
}
attr {
  key: "padding"
  value {
    s: "VALID"
  }
}
attr {
  key: "strides"
  value {
    list {
      i: 1
      i: 2
      i: 2
      i: 1
    }
  }
}
 for consumer [<tf.Operation 'training/RMSprop/gradients/gradients/block3_conv2/Relu_grad/ReluGrad' type=ReluGrad>, <tf.Operation 'training/RMSprop/gradients/gradients/block3_conv3/Conv2D_grad/Conv2DBackpropFilter' type=Conv2DBackpropFilter>]
[STR DEBUG] Add swap-in op at name: "training/RMSprop/gradients/gradients/block3_pool/MaxPool_grad/MaxPoolGrad"
op: "MaxPoolGrad"
input: "block3_conv3/Relu"
input: "block3_pool/MaxPool"
input: "training/RMSprop/gradients/gradients/block4_conv1/Conv2D_grad/Conv2DBackpropInput"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
attr {
  key: "data_format"
  value {
    s: "NHWC"
  }
}
attr {
  key: "ksize"
  value {
    list {
      i: 1
      i: 2
      i: 2
      i: 1
    }
  }
}
attr {
  key: "padding"
  value {
    s: "VALID"
  }
}
attr {
  key: "strides"
  value {
    list {
      i: 1
      i: 2
      i: 2
      i: 1
    }
  }
}
 for consumer [<tf.Operation 'training/RMSprop/gradients/gradients/block3_conv2/Relu_grad/ReluGrad' type=ReluGrad>, <tf.Operation 'training/RMSprop/gradients/gradients/block3_conv3/Conv2D_grad/Conv2DBackpropFilter' type=Conv2DBackpropFilter>]
[STR DEBUG] Processing layer 10's swapping, swap out at [11], swap in at [33]
[STR DEBUG] Find swapout ops: name: "block3_pool/MaxPool"
op: "MaxPool"
input: "block3_conv3/Relu"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
attr {
  key: "data_format"
  value {
    s: "NHWC"
  }
}
attr {
  key: "ksize"
  value {
    list {
      i: 1
      i: 2
      i: 2
      i: 1
    }
  }
}
attr {
  key: "padding"
  value {
    s: "VALID"
  }
}
attr {
  key: "strides"
  value {
    list {
      i: 1
      i: 2
      i: 2
      i: 1
    }
  }
}
, 
	choose swap tensor Tensor("block3_pool/MaxPool:0", shape=(?, 28, 28, 256), dtype=float32), 
	finish at the end of ops: [<tf.Operation 'block4_conv1/Relu' type=Relu>]
[STR DEBUG] Add swap-in op at name: "training/RMSprop/gradients/gradients/block4_conv1/Relu_grad/ReluGrad"
op: "ReluGrad"
input: "training/RMSprop/gradients/gradients/block4_conv2/Conv2D_grad/Conv2DBackpropInput"
input: "block4_conv1/Relu"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
 for consumer [<tf.Operation 'training/RMSprop/gradients/gradients/block3_pool/MaxPool_grad/MaxPoolGrad' type=MaxPoolGrad>, <tf.Operation 'training/RMSprop/gradients/gradients/block4_conv1/Conv2D_grad/Conv2DBackpropFilter' type=Conv2DBackpropFilter>]
[STR DEBUG] Processing layer 11's swapping, swap out at [12], swap in at [32]
[STR DEBUG] Find swapout ops: name: "block4_conv1/Relu"
op: "Relu"
input: "block4_conv1/BiasAdd"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
, 
	choose swap tensor Tensor("block4_conv1/Relu:0", shape=(?, 28, 28, 512), dtype=float32), 
	finish at the end of ops: [<tf.Operation 'block4_conv2/Relu' type=Relu>]
[STR DEBUG] Add swap-in op at name: "training/RMSprop/gradients/gradients/block4_conv2/Relu_grad/ReluGrad"
op: "ReluGrad"
input: "training/RMSprop/gradients/gradients/block4_conv3/Conv2D_grad/Conv2DBackpropInput"
input: "block4_conv2/Relu"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
 for consumer [<tf.Operation 'training/RMSprop/gradients/gradients/block4_conv1/Relu_grad/ReluGrad' type=ReluGrad>, <tf.Operation 'training/RMSprop/gradients/gradients/block4_conv2/Conv2D_grad/Conv2DBackpropFilter' type=Conv2DBackpropFilter>]
[STR DEBUG] Processing layer 12's swapping, swap out at [14], swap in at [30]
[STR DEBUG] Find swapout ops: name: "block4_conv2/Relu"
op: "Relu"
input: "block4_conv2/BiasAdd"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
, 
	choose swap tensor Tensor("block4_conv2/Relu:0", shape=(?, 28, 28, 512), dtype=float32), 
	finish at the end of ops: [<tf.Operation 'block4_pool/MaxPool' type=MaxPool>]
[STR DEBUG] Add swap-in op at name: "training/RMSprop/gradients/gradients/block4_pool/MaxPool_grad/MaxPoolGrad"
op: "MaxPoolGrad"
input: "block4_conv3/Relu"
input: "block4_pool/MaxPool"
input: "training/RMSprop/gradients/gradients/block5_conv1/Conv2D_grad/Conv2DBackpropInput"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
attr {
  key: "data_format"
  value {
    s: "NHWC"
  }
}
attr {
  key: "ksize"
  value {
    list {
      i: 1
      i: 2
      i: 2
      i: 1
    }
  }
}
attr {
  key: "padding"
  value {
    s: "VALID"
  }
}
attr {
  key: "strides"
  value {
    list {
      i: 1
      i: 2
      i: 2
      i: 1
    }
  }
}
 for consumer [<tf.Operation 'training/RMSprop/gradients/gradients/block4_conv3/Conv2D_grad/Conv2DBackpropFilter' type=Conv2DBackpropFilter>, <tf.Operation 'training/RMSprop/gradients/gradients/block4_conv2/Relu_grad/ReluGrad' type=ReluGrad>]
[STR DEBUG] Processing layer 14's swapping, swap out at [15], swap in at [29]
[STR DEBUG] Find swapout ops: name: "block4_pool/MaxPool"
op: "MaxPool"
input: "block4_conv3/Relu"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
attr {
  key: "data_format"
  value {
    s: "NHWC"
  }
}
attr {
  key: "ksize"
  value {
    list {
      i: 1
      i: 2
      i: 2
      i: 1
    }
  }
}
attr {
  key: "padding"
  value {
    s: "VALID"
  }
}
attr {
  key: "strides"
  value {
    list {
      i: 1
      i: 2
      i: 2
      i: 1
    }
  }
}
, 
	choose swap tensor Tensor("block4_pool/MaxPool:0", shape=(?, 14, 14, 512), dtype=float32), 
	finish at the end of ops: [<tf.Operation 'block5_conv1/Relu' type=Relu>]
[STR DEBUG] Add swap-in op at name: "training/RMSprop/gradients/gradients/block5_conv1/Relu_grad/ReluGrad"
op: "ReluGrad"
input: "training/RMSprop/gradients/gradients/block5_conv2/Conv2D_grad/Conv2DBackpropInput"
input: "block5_conv1/Relu"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
 for consumer [<tf.Operation 'training/RMSprop/gradients/gradients/block4_pool/MaxPool_grad/MaxPoolGrad' type=MaxPoolGrad>, <tf.Operation 'training/RMSprop/gradients/gradients/block5_conv1/Conv2D_grad/Conv2DBackpropFilter' type=Conv2DBackpropFilter>]
[STR DEBUG] Processing layer 15's swapping, swap out at [16], swap in at [28]
[STR DEBUG] Find swapout ops: name: "block5_conv1/Relu"
op: "Relu"
input: "block5_conv1/BiasAdd"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
, 
	choose swap tensor Tensor("block5_conv1/Relu:0", shape=(?, 14, 14, 512), dtype=float32), 
	finish at the end of ops: [<tf.Operation 'block5_conv2/Relu' type=Relu>]
[STR DEBUG] Add swap-in op at name: "training/RMSprop/gradients/gradients/block5_conv2/Relu_grad/ReluGrad"
op: "ReluGrad"
input: "training/RMSprop/gradients/gradients/block5_conv3/Conv2D_grad/Conv2DBackpropInput"
input: "block5_conv2/Relu"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
 for consumer [<tf.Operation 'training/RMSprop/gradients/gradients/block5_conv1/Relu_grad/ReluGrad' type=ReluGrad>, <tf.Operation 'training/RMSprop/gradients/gradients/block5_conv2/Conv2D_grad/Conv2DBackpropFilter' type=Conv2DBackpropFilter>]
[STR DEBUG] Processing layer 16's swapping, swap out at [19], swap in at [25]
[STR DEBUG] Cannot find swap-out control nodes for layer 16, use the first op name: "flatten/Shape"
op: "Shape"
input: "block5_pool/MaxPool"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
attr {
  key: "out_type"
  value {
    type: DT_INT32
  }
}

[STR DEBUG] Find swapout ops: name: "block5_conv2/Relu"
op: "Relu"
input: "block5_conv2/BiasAdd"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
, 
	choose swap tensor Tensor("block5_conv2/Relu:0", shape=(?, 14, 14, 512), dtype=float32), 
	finish at the end of ops: [<tf.Operation 'flatten/Shape' type=Shape>]2022-01-08 23:41:35.502327: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.68GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2022-01-08 23:41:35.502397: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.68GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2022-01-08 23:41:35.544370: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.64GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.

[STR DEBUG] Cannot find swap-in control nodes for ref. layer 25, use the last op name: "training/RMSprop/gradients/gradients/flatten/Reshape_grad/Reshape"
op: "Reshape"
input: "training/RMSprop/gradients/gradients/fc1/MatMul_grad/MatMul"
input: "training/RMSprop/gradients/gradients/flatten/Reshape_grad/Shape"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
attr {
  key: "Tshape"
  value {
    type: DT_INT32
  }
}

[STR DEBUG] Add swap-in op at name: "training/RMSprop/gradients/gradients/flatten/Reshape_grad/Reshape"
op: "Reshape"
input: "training/RMSprop/gradients/gradients/fc1/MatMul_grad/MatMul"
input: "training/RMSprop/gradients/gradients/flatten/Reshape_grad/Shape"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
attr {
  key: "Tshape"
  value {
    type: DT_INT32
  }
}
 for consumer [<tf.Operation 'training/RMSprop/gradients/gradients/block5_conv2/Relu_grad/ReluGrad' type=ReluGrad>, <tf.Operation 'training/RMSprop/gradients/gradients/block5_conv3/Conv2D_grad/Conv2DBackpropFilter' type=Conv2DBackpropFilter>]
[STR DEBUG] Processing layer 19's swapping, swap out at [20], swap in at [24]
[STR DEBUG] Cannot find swap-out activation nodes for layer 19, among [<tf.Operation 'flatten/Shape' type=Shape>, <tf.Operation 'flatten/strided_slice/stack_2' type=Const>, <tf.Operation 'flatten/strided_slice' type=StridedSlice>, <tf.Operation 'flatten/Reshape/shape/1' type=Const>, <tf.Operation 'flatten/Reshape' type=Reshape>, <tf.Operation 'flatten/Reshape/shape' type=Pack>, <tf.Operation 'flatten/strided_slice/stack_1' type=Const>, <tf.Operation 'flatten/strided_slice/stack' type=Const>]
[STR DEBUG] Choose op that have outputs: name: "flatten/strided_slice/stack"
op: "Const"
attr {
  key: "dtype"
  value {
    type: DT_INT32
  }
}
attr {
  key: "value"
  value {
    tensor {
      dtype: DT_INT32
      tensor_shape {
        dim {
          size: 1
        }
      }
      int_val: 0
    }
  }
}

[STR DEBUG] Find swapout ops: name: "flatten/strided_slice/stack"
op: "Const"
attr {
  key: "dtype"
  value {
    type: DT_INT32
  }
}
attr {
  key: "value"
  value {
    tensor {
      dtype: DT_INT32
      tensor_shape {
        dim {
          size: 1
        }
      }
      int_val: 0
    }
  }
}
, 
	choose swap tensor Tensor("flatten/strided_slice/stack:0", shape=(1,), dtype=int32), 
	finish at the end of ops: [<tf.Operation 'fc1/Relu' type=Relu>]
[STR DEBUG] Processing layer 20's swapping, swap out at [21], swap in at [23]
[STR DEBUG] Find swapout ops: name: "fc1/Relu"
op: "Relu"
input: "fc1/BiasAdd"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
, 
	choose swap tensor Tensor("fc1/Relu:0", shape=(?, 4096), dtype=float32), 
	finish at the end of ops: [<tf.Operation 'fc2/Relu' type=Relu>]
[STR DEBUG] Add swap-in op at name: "training/RMSprop/gradients/gradients/fc2/Relu_grad/ReluGrad"
op: "ReluGrad"
input: "training/RMSprop/gradients/gradients/predictions/MatMul_grad/MatMul"
input: "fc2/Relu"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
 for consumer [<tf.Operation 'training/RMSprop/gradients/gradients/fc1/Relu_grad/ReluGrad' type=ReluGrad>, <tf.Operation 'training/RMSprop/gradients/gradients/fc2/MatMul_grad/MatMul_1' type=MatMul>]
[STR DEBUG] Cannot find swap-out operations for swap-in at layer 21, skipping
  1/149 [..............................] - ETA: 1:03:10 - loss: 2.3743 - acc: 0.09262022-01-08 23:41:41.706578: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.64GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
  2/149 [..............................] - ETA: 33:34 - loss: 6119927.6872 - acc: 0.09442022-01-08 23:41:43.437755: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.64GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
  3/149 [..............................] - ETA: 23:37 - loss: 4079959.2802 - acc: 0.09142022-01-08 23:41:45.218718: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.64GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2022-01-08 23:41:45.219623: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.64GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
  4/149 [..............................] - ETA: 18:42 - loss: 3059982.1716 - acc: 0.10372022-01-08 23:41:47.032919: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.64GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2022-01-08 23:41:47.033886: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.64GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2022-01-08 23:41:47.034748: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.73GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
  5/149 [>.............................] - ETA: 15:46 - loss: 2448325.2308 - acc: 0.1037  6/149 [>.............................] - ETA: 13:44 - loss: 2040273.1195 - acc: 0.1012  7/149 [>.............................] - ETA: 12:21 - loss: 1748806.0960 - acc: 0.0989  8/149 [>.............................] - ETA: 11:18 - loss: 1530207.6586 - acc: 0.0995  9/149 [>.............................] - ETA: 10:30 - loss: 1360184.9697 - acc: 0.1000 10/149 [=>............................] - ETA: 9:49 - loss: 1224166.7229 - acc: 0.1000  11/149 [=>............................] - ETA: 9:17 - loss: 1112879.0479 - acc: 0.0987 12/149 [=>............................] - ETA: 8:50 - loss: 1020139.3190 - acc: 0.1000 13/149 [=>............................] - ETA: 8:26 - loss: 941667.2412 - acc: 0.1006  14/149 [=>............................] - ETA: 8:06 - loss: 874405.4597 - acc: 0.0997 15/149 [==>...........................] - ETA: 7:48 - loss: 816111.9203 - acc: 0.1025 16/149 [==>...........................] - ETA: 7:32 - loss: 765105.0706 - acc: 0.1012 17/149 [==>...........................] - ETA: 7:19 - loss: 720099.0565 - acc: 0.1017 18/149 [==>...........................] - ETA: 7:07 - loss: 680093.6814 - acc: 0.1014 19/149 [==>...........................] - ETA: 6:56 - loss: 644299.3984 - acc: 0.1010 20/149 [===>..........................] - ETA: 6:46 - loss: 612084.5433 - acc: 0.1024 21/149 [===>..........................] - ETA: 6:38 - loss: 582937.7697 - acc: 0.1037 22/149 [===>..........................] - ETA: 6:29 - loss: 556440.7031 - acc: 0.1039 23/149 [===>..........................] - ETA: 6:21 - loss: 532247.7288 - acc: 0.1034 24/149 [===>..........................] - ETA: 6:14 - loss: 510070.8370 - acc: 0.1032 25/149 [====>.........................] - ETA: 6:06 - loss: 489668.0956 - acc: 0.1037 26/149 [====>.........................] - ETA: 5:59 - loss: 470834.7953 - acc: 0.1038 27/149 [====>.........................] - ETA: 5:52 - loss: 453396.5545 - acc: 0.1030 28/149 [====>.........................] - ETA: 5:45 - loss: 437203.9036 - acc: 0.1020 29/149 [====>.........................] - ETA: 5:39 - loss: 422127.9861 - acc: 0.1024 30/149 [=====>........................] - ETA: 5:34 - loss: 408057.1296 - acc: 0.1022 31/149 [=====>........................] - ETA: 5:27 - loss: 394894.0703 - acc: 0.1025 32/149 [=====>........................] - ETA: 5:22 - loss: 382553.7017 - acc: 0.1044 33/149 [=====>........................] - ETA: 5:17 - loss: 370961.2339 - acc: 0.1046 34/149 [=====>........................] - ETA: 5:11 - loss: 360050.6757 - acc: 0.1058 35/149 [======>.......................] - ETA: 5:06 - loss: 349763.5811 - acc: 0.1053 36/149 [======>.......................] - ETA: 5:02 - loss: 340047.9900 - acc: 0.1059 37/149 [======>.......................] - ETA: 4:58 - loss: 330857.5657 - acc: 0.1079 38/149 [======>.......................] - ETA: 4:53 - loss: 322150.8480 - acc: 0.1077 39/149 [======>.......................] - ETA: 4:49 - loss: 313890.6286 - acc: 0.1073 40/149 [=======>......................] - ETA: 4:44 - loss: 306043.4200 - acc: 0.1081 41/149 [=======>......................] - ETA: 4:40 - loss: 298579.0022 - acc: 0.1097 42/149 [=======>......................] - ETA: 4:37 - loss: 291470.0348 - acc: 0.1092 43/149 [=======>......................] - ETA: 4:33 - loss: 284691.7155 - acc: 0.1095 44/149 [=======>......................] - ETA: 4:30 - loss: 278221.5016 - acc: 0.1094 45/149 [========>.....................] - ETA: 4:26 - loss: 272038.8527 - acc: 0.1096 46/149 [========>.....................] - ETA: 4:22 - loss: 266125.0147 - acc: 0.1093 47/149 [========>.....................] - ETA: 4:19 - loss: 260462.8293 - acc: 0.1091 48/149 [========>.....................] - ETA: 4:15 - loss: 255036.5683 - acc: 0.1086 49/149 [========>.....................] - ETA: 4:11 - loss: 249831.7872 - acc: 0.1082 50/149 [=========>....................] - ETA: 4:08 - loss: 244835.1971 - acc: 0.1084 51/149 [=========>....................] - ETA: 4:05 - loss: 240034.5515 - acc: 0.1081 52/149 [=========>....................] - ETA: 4:01 - loss: 235418.5535 - acc: 0.1083 53/149 [=========>....................] - ETA: 3:58 - loss: 230976.7374 - acc: 0.1087 54/149 [=========>....................] - ETA: 3:55 - loss: 226699.4329 - acc: 0.1083 55/149 [==========>...................] - ETA: 3:52 - loss: 222577.6667 - acc: 0.1082 56/149 [==========>...................] - ETA: 3:48 - loss: 218603.1060 - acc: 0.1088 57/149 [==========>...................] - ETA: 3:45 - loss: 214768.0040 - acc: 0.1092 58/149 [==========>...................] - ETA: 3:42 - loss: 211065.1555 - acc: 0.1099 59/149 [==========>...................] - ETA: 3:39 - loss: 207487.8667 - acc: 0.1099 60/149 [===========>..................] - ETA: 3:36 - loss: 204029.7742 - acc: 0.1097 61/149 [===========>..................] - ETA: 3:33 - loss: 200685.0617 - acc: 0.1096 62/149 [===========>..................] - ETA: 3:30 - loss: 197448.2430 - acc: 0.1093 63/149 [===========>..................] - ETA: 3:27 - loss: 194314.1805 - acc: 0.1089 64/149 [===========>..................] - ETA: 3:24 - loss: 191278.0574 - acc: 0.1087 65/149 [============>.................] - ETA: 3:21 - loss: 188335.3535 - acc: 0.1087 66/149 [============>.................] - ETA: 3:18 - loss: 185481.8225 - acc: 0.1086 67/149 [============>.................] - ETA: 3:15 - loss: 182713.4715 - acc: 0.1086 68/149 [============>.................] - ETA: 3:12 - loss: 180026.5425 - acc: 0.1087 69/149 [============>.................] - ETA: 3:09 - loss: 177417.4955 - acc: 0.1088 70/149 [=============>................] - ETA: 3:06 - loss: 174882.9934 - acc: 0.1088 71/149 [=============>................] - ETA: 3:04 - loss: 172419.8851 - acc: 0.1086 72/149 [=============>................] - ETA: 3:03 - loss: 170025.1964 - acc: 0.1086 73/149 [=============>................] - ETA: 3:00 - loss: 167696.1156 - acc: 0.1086 74/149 [=============>................] - ETA: 2:57 - loss: 165429.9827 - acc: 0.1084 75/149 [==============>...............] - ETA: 2:55 - loss: 163224.2951 - acc: 0.1080 76/149 [==============>...............] - ETA: 2:52 - loss: 161076.6373 - acc: 0.1083 77/149 [==============>...............] - ETA: 2:49 - loss: 158984.7635 - acc: 0.1088 78/149 [==============>...............] - ETA: 2:46 - loss: 156946.5554 - acc: 0.1086 79/149 [==============>...............] - ETA: 2:44 - loss: 154959.9192 - acc: 0.1090 80/149 [===============>..............] - ETA: 2:41 - loss: 153022.9490 - acc: 0.1088 81/149 [===============>..............] - ETA: 2:39 - loss: 151133.8048 - acc: 0.1093 82/149 [===============>..............] - ETA: 2:36 - loss: 149290.7378 - acc: 0.1093 83/149 [===============>..............] - ETA: 2:34 - loss: 147492.0818 - acc: 0.1096 84/149 [===============>..............] - ETA: 2:31 - loss: 145736.2505 - acc: 0.1099 85/149 [================>.............] - ETA: 2:29 - loss: 144021.7335 - acc: 0.1094 86/149 [================>.............] - ETA: 2:26 - loss: 142347.0890 - acc: 0.1093 87/149 [================>.............] - ETA: 2:23 - loss: 140710.9419 - acc: 0.1097 88/149 [================>.............] - ETA: 2:21 - loss: 139111.9802 - acc: 0.1102 89/149 [================>.............] - ETA: 2:18 - loss: 137548.9501 - acc: 0.1098 90/149 [=================>............] - ETA: 2:16 - loss: 136020.6557 - acc: 0.1096 91/149 [=================>............] - ETA: 2:13 - loss: 134525.9484 - acc: 0.1100 92/149 [=================>............] - ETA: 2:11 - loss: 133063.7345 - acc: 0.1102 93/149 [=================>............] - ETA: 2:08 - loss: 131632.9680 - acc: 0.1103 94/149 [=================>............] - ETA: 2:06 - loss: 130232.6418 - acc: 0.1102 95/149 [==================>...........] - ETA: 2:03 - loss: 128861.7961 - acc: 0.1102 96/149 [==================>...........] - ETA: 2:01 - loss: 127519.5097 - acc: 0.1100 97/149 [==================>...........] - ETA: 1:59 - loss: 126204.8993 - acc: 0.1100 98/149 [==================>...........] - ETA: 1:56 - loss: 124917.1180 - acc: 0.1100 99/149 [==================>...........] - ETA: 1:54 - loss: 123655.3522 - acc: 0.1097100/149 [===================>..........] - ETA: 1:51 - loss: 122418.8217 - acc: 0.1097101/149 [===================>..........] - ETA: 1:49 - loss: 121206.7770 - acc: 0.1095102/149 [===================>..........] - ETA: 1:46 - loss: 120018.4978 - acc: 0.1095103/149 [===================>..........] - ETA: 1:44 - loss: 118853.2920 - acc: 0.1096104/149 [===================>..........] - ETA: 1:42 - loss: 117710.4940 - acc: 0.1096105/149 [====================>.........] - ETA: 1:39 - loss: 116589.4636 - acc: 0.1097106/149 [====================>.........] - ETA: 1:37 - loss: 115489.5846 - acc: 0.1099107/149 [====================>.........] - ETA: 1:34 - loss: 114410.2642 - acc: 0.1098108/149 [====================>.........] - ETA: 1:32 - loss: 113350.9310 - acc: 0.1099109/149 [====================>.........] - ETA: 1:30 - loss: 112311.0353 - acc: 0.1100110/149 [=====================>........] - ETA: 1:27 - loss: 111290.0469 - acc: 0.1100111/149 [=====================>........] - ETA: 1:25 - loss: 110287.4546 - acc: 0.1097112/149 [=====================>........] - ETA: 1:23 - loss: 109302.7658 - acc: 0.1093113/149 [=====================>........] - ETA: 1:20 - loss: 108335.5050 - acc: 0.1093114/149 [=====================>........] - ETA: 1:18 - loss: 107385.2138 - acc: 0.1095115/149 [======================>.......] - ETA: 1:16 - loss: 106451.4493 - acc: 0.1092116/149 [======================>.......] - ETA: 1:13 - loss: 105533.7843 - acc: 0.1092117/149 [======================>.......] - ETA: 1:11 - loss: 104631.8058 - acc: 0.1089118/149 [======================>.......] - ETA: 1:09 - loss: 103745.1151 - acc: 0.1092119/149 [======================>.......] - ETA: 1:06 - loss: 102873.3296 - acc: 0.1091120/149 [=======================>......] - ETA: 1:04 - loss: 102016.0710 - acc: 0.1092121/149 [=======================>......] - ETA: 1:02 - loss: 101172.9820 - acc: 0.1092122/149 [=======================>......] - ETA: 1:00 - loss: 100343.7141 - acc: 0.1094123/149 [=======================>......] - ETA: 57s - loss: 99527.9301 - acc: 0.1096  124/149 [=======================>......] - ETA: 55s - loss: 98725.3179 - acc: 0.1097125/149 [========================>.....] - ETA: 53s - loss: 97935.5337 - acc: 0.1097126/149 [========================>.....] - ETA: 51s - loss: 97158.2858 - acc: 0.1097127/149 [========================>.....] - ETA: 48s - loss: 96393.2781 - acc: 0.1096128/149 [========================>.....] - ETA: 46s - loss: 95640.2236 - acc: 0.1096129/149 [========================>.....] - ETA: 44s - loss: 94898.8443 - acc: 0.1095130/149 [=========================>....] - ETA: 41s - loss: 94168.8709 - acc: 0.1094131/149 [=========================>....] - ETA: 39s - loss: 93450.0422 - acc: 0.1094132/149 [=========================>....] - ETA: 37s - loss: 92742.1047 - acc: 0.1093133/149 [=========================>....] - ETA: 35s - loss: 92044.8129 - acc: 0.1094134/149 [=========================>....] - ETA: 33s - loss: 91357.9286 - acc: 0.1094135/149 [==========================>...] - ETA: 30s - loss: 90681.2202 - acc: 0.1091136/149 [==========================>...] - ETA: 28s - loss: 90014.4635 - acc: 0.1090137/149 [==========================>...] - ETA: 26s - loss: 89357.4408 - acc: 0.1091138/149 [==========================>...] - ETA: 24s - loss: 88709.9397 - acc: 0.1097139/149 [==========================>...] - ETA: 21s - loss: 88071.7595 - acc: 0.1097140/149 [===========================>..] - ETA: 19s - loss: 87442.6921 - acc: 0.1097141/149 [===========================>..] - ETA: 17s - loss: 86822.5474 - acc: 0.1097142/149 [===========================>..] - ETA: 15s - loss: 86211.1372 - acc: 0.1095143/149 [===========================>..] - ETA: 13s - loss: 85608.2782 - acc: 0.1096144/149 [===========================>..] - ETA: 10s - loss: 85013.7928 - acc: 0.1095145/149 [============================>.] - ETA: 8s - loss: 84427.5068 - acc: 0.1095 146/149 [============================>.] - ETA: 6s - loss: 83849.2520 - acc: 0.1093147/149 [============================>.] - ETA: 4s - loss: 83278.8653 - acc: 0.1091148/149 [============================>.] - ETA: 2s - loss: 82716.1859 - acc: 0.1089  1/149 [..............................] - ETA: 2:22 - loss: 2.2800 - acc: 0.1000  2/149 [..............................] - ETA: 1:54 - loss: 2.2807 - acc: 0.1130  3/149 [..............................] - ETA: 1:45 - loss: 2.2887 - acc: 0.1099  4/149 [..............................] - ETA: 1:38 - loss: 2.2821 - acc: 0.1139  5/149 [>.............................] - ETA: 1:35 - loss: 2.2832 - acc: 0.1163  6/149 [>.............................] - ETA: 1:32 - loss: 2.2843 - acc: 0.1241  7/149 [>.............................] - ETA: 1:30 - loss: 2.2844 - acc: 0.1249  8/149 [>.............................] - ETA: 1:29 - loss: 2.2821 - acc: 0.1282  9/149 [>.............................] - ETA: 1:28 - loss: 2.2809 - acc: 0.1263 10/149 [=>............................] - ETA: 1:27 - loss: 2.2821 - acc: 0.1237 11/149 [=>............................] - ETA: 1:26 - loss: 2.2810 - acc: 0.1242 12/149 [=>............................] - ETA: 1:26 - loss: 2.2817 - acc: 0.1228 13/149 [=>............................] - ETA: 1:25 - loss: 2.2820 - acc: 0.1214 14/149 [=>............................] - ETA: 1:24 - loss: 2.2818 - acc: 0.1238 15/149 [==>...........................] - ETA: 1:24 - loss: 2.2819 - acc: 0.1235 16/149 [==>...........................] - ETA: 1:24 - loss: 2.2817 - acc: 0.1243 17/149 [==>...........................] - ETA: 1:24 - loss: 2.2814 - acc: 0.1242 18/149 [==>...........................] - ETA: 1:24 - loss: 2.2814 - acc: 0.1257 19/149 [==>...........................] - ETA: 1:24 - loss: 2.2811 - acc: 0.1265 20/149 [===>..........................] - ETA: 1:24 - loss: 2.2811 - acc: 0.1280 21/149 [===>..........................] - ETA: 1:24 - loss: 2.2816 - acc: 0.1284 22/149 [===>..........................] - ETA: 1:24 - loss: 2.2821 - acc: 0.1295 23/149 [===>..........................] - ETA: 1:24 - loss: 2.2820 - acc: 0.1295 24/149 [===>..........................] - ETA: 1:24 - loss: 2.2817 - acc: 0.1287 25/149 [====>.........................] - ETA: 1:23 - loss: 2.2815 - acc: 0.1290 26/149 [====>.........................] - ETA: 1:22 - loss: 2.2810 - acc: 0.1303 27/149 [====>.........................] - ETA: 1:21 - loss: 2.2810 - acc: 0.1310 28/149 [====>.........................] - ETA: 1:20 - loss: 2.2809 - acc: 0.1307 29/149 [====>.........................] - ETA: 1:19 - loss: 2.2803 - acc: 0.1313 30/149 [=====>........................] - ETA: 1:18 - loss: 2.2800 - acc: 0.1315 31/149 [=====>........................] - ETA: 1:17 - loss: 2.2800 - acc: 0.1319 32/149 [=====>........................] - ETA: 1:16 - loss: 2.2796 - acc: 0.1329 33/149 [=====>........................] - ETA: 1:16 - loss: 2.2799 - acc: 0.1322 34/149 [=====>........................] - ETA: 1:15 - loss: 2.2806 - acc: 0.1315 35/149 [======>.......................] - ETA: 1:14 - loss: 2.2807 - acc: 0.1311 36/149 [======>.......................] - ETA: 1:13 - loss: 2.2808 - acc: 0.1315 37/149 [======>.......................] - ETA: 1:13 - loss: 2.2806 - acc: 0.1311 38/149 [======>.......................] - ETA: 1:12 - loss: 2.2776 - acc: 0.1312149/149 [==============================] - 349s 2s/step - loss: 82164.2086 - acc: 0.1086 - val_loss: 2.2776 - val_acc: 0.1312

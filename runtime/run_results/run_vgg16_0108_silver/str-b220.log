WARNING:tensorflow:From /home/zongzan/.conda/envs/STR36/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
WARNING:tensorflow:
The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
  * https://github.com/tensorflow/io (for I/O related ops)
If you depend on functionality not listed there, please file an issue.

WARNING:tensorflow:From /home/zongzan/.conda/envs/STR36/lib/python3.6/site-packages/tensorflow_core/python/ops/stropt/lms.py:970: The name tf.train.SessionRunHook is deprecated. Please use tf.estimator.SessionRunHook instead.

WARNING:tensorflow:From /home/zongzan/.conda/envs/STR36/lib/python3.6/site-packages/tensorflow_core/python/ops/stropt/lms.py:928: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.

WARNING:tensorflow:From /home/zongzan/.conda/envs/STR36/lib/python3.6/site-packages/tensorflow_core/contrib/graph_editor/select.py:554: get_backward_walk_ops (from tensorflow.contrib.graph_editor.select) is deprecated and will be removed after 2019-06-06.
Instructions for updating:
Please use tensorflow.python.ops.op_selector.get_backward_walk_ops.
1 Physical GPUs, 1 Logical GPUs
Parsing arguments: [('batch_size', 220), ('epochs', 1), ('model_name', 'VGG16'), ('strategy', 'str'), ('verbose', False)]
Cannot find config for batch_size=220, use 250 instead
Shape of x_train: (40000, 224, 224, 3), shape of x_test: (10000, 224, 224, 3)
[STR DEBUG] Parsing STR configuration: [{'strategy': 'hybrid', 'verbose': 'false', 'tags': '/home/zongzan/dist_dnn_training/STR/optimizer/logs/VGG16_250/layer_names'}, {'r': '/home/zongzan/dist_dnn_training/STR/optimizer/logs/VGG16_250/R', 'p': '/home/zongzan/dist_dnn_training/STR/optimizer/logs/VGG16_250/P', 'q': '/home/zongzan/dist_dnn_training/STR/optimizer/logs/VGG16_250/PrunedQ'}]
[STR DEBUG] Processing layer 0's swapping, swap out at [1], swap in at [43]
[STR DEBUG] Find swapout ops: name: "input_1"
op: "Placeholder"
attr {
  key: "dtype"
  value {
    type: DT_FLOAT
  }
}
attr {
  key: "shape"
  value {
    shape {
      dim {
        size: -1
      }
      dim {
        size: 224
      }
      dim {
        size: 224
      }
      dim {
        size: 3
      }
    }
  }
}
, 
	choose swap tensor Tensor("input_1:0", shape=(?, 224, 224, 3), dtype=float32), 
	finish at the end of ops: [<tf.Operation 'block1_conv1/Relu' type=Relu>]
[STR DEBUG] Add swap-in op at name: "training/RMSprop/gradients/gradients/block1_conv1/Relu_grad/ReluGrad"
op: "ReluGrad"
input: "training/RMSprop/gradients/gradients/block1_conv2/Conv2D_grad/Conv2DBackpropInput"
input: "block1_conv1/Relu"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
 for consumer [<tf.Operation 'training/RMSprop/gradients/gradients/block1_conv1/Conv2D_grad/Conv2DBackpropFilter' type=Conv2DBackpropFilter>]
[STR DEBUG] Processing layer 1's swapping, swap out at [3], swap in at [8, 41]
[STR DEBUG] Find swapout ops: name: "block1_conv1/Relu"
op: "Relu"
input: "block1_conv1/BiasAdd"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
, 
	choose swap tensor Tensor("block1_conv1/Relu:0", shape=(?, 224, 224, 64), dtype=float32), 
	finish at the end of ops: [<tf.Operation 'block1_pool/MaxPool' type=MaxPool>]
[STR DEBUG] Add swap-in op at name: "block3_conv2/Relu"
op: "Relu"
input: "block3_conv2/BiasAdd"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
 for consumer [<tf.Operation 'training/RMSprop/gradients/gradients/block1_conv2/Conv2D_grad/Conv2DBackpropFilter' type=Conv2DBackpropFilter>, <tf.Operation 'training/RMSprop/gradients/gradients/block1_conv1/Relu_grad/ReluGrad' type=ReluGrad>]
[STR DEBUG] Add swap-in op at name: "training/RMSprop/gradients/gradients/block1_pool/MaxPool_grad/MaxPoolGrad"
op: "MaxPoolGrad"
input: "block1_conv2/Relu"
input: "block1_pool/MaxPool"
input: "training/RMSprop/gradients/gradients/block2_conv1/Conv2D_grad/Conv2DBackpropInput"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
attr {
  key: "data_format"
  value {
    s: "NHWC"
  }
}
attr {
  key: "ksize"
  value {
    list {
      i: 1
      i: 2
      i: 2
      i: 1
    }
  }
}
attr {
  key: "padding"
  value {
    s: "VALID"
  }
}
attr {
  key: "strides"
  value {
    list {
      i: 1
      i: 2
      i: 2
      i: 1
    }
  }
}
 for consumer [<tf.Operation 'training/RMSprop/gradients/gradients/block1_conv2/Conv2D_grad/Conv2DBackpropFilter' type=Conv2DBackpropFilter>, <tf.Operation 'training/RMSprop/gradients/gradients/block1_conv1/Relu_grad/ReluGrad' type=ReluGrad>]
[STR DEBUG] Processing layer 3's swapping, swap out at [4], swap in at [40]
[STR DEBUG] Find swapout ops: name: "block1_pool/MaxPool"
op: "MaxPool"
input: "block1_conv2/Relu"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
attr {
  key: "data_format"
  value {
    s: "NHWC"
  }
}
attr {
  key: "ksize"
  value {
    list {
      i: 1
      i: 2
      i: 2
      i: 1
    }
  }
}
attr {
  key: "padding"
  value {
    s: "VALID"
  }
}
attr {
  key: "strides"
  value {
    list {
      i: 1
      i: 2
      i: 2
      i: 1
    }
  }
}
, 
	choose swap tensor Tensor("block1_pool/MaxPool:0", shape=(?, 112, 112, 64), dtype=float32), 
	finish at the end of ops: [<tf.Operation 'block2_conv1/Relu' type=Relu>]
[STR DEBUG] Add swap-in op at name: "training/RMSprop/gradients/gradients/block2_conv1/Relu_grad/ReluGrad"
op: "ReluGrad"
input: "training/RMSprop/gradients/gradients/block2_conv2/Conv2D_grad/Conv2DBackpropInput"
input: "block2_conv1/Relu"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
 for consumer [<tf.Operation 'training/RMSprop/gradients/gradients/block1_pool/MaxPool_grad/MaxPoolGrad' type=MaxPoolGrad>, <tf.Operation 'training/RMSprop/gradients/gradients/block2_conv1/Conv2D_grad/Conv2DBackpropFilter' type=Conv2DBackpropFilter>]
[STR DEBUG] Processing layer 4's swapping, swap out at [6], swap in at [11, 38]
[STR DEBUG] Find swapout ops: name: "block2_conv1/Relu"
op: "Relu"
input: "block2_conv1/BiasAdd"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
, 
	choose swap tensor Tensor("block2_conv1/Relu:0", shape=(?, 112, 112, 128), dtype=float32), 
	finish at the end of ops: [<tf.Operation 'block2_pool/MaxPool' type=MaxPool>]
[STR DEBUG] Add swap-in op at name: "block4_conv1/Relu"
op: "Relu"
input: "block4_conv1/BiasAdd"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
 for consumer [<tf.Operation 'training/RMSprop/gradients/gradients/block2_conv2/Conv2D_grad/Conv2DBackpropFilter' type=Conv2DBackpropFilter>, <tf.Operation 'training/RMSprop/gradients/gradients/block2_conv1/Relu_grad/ReluGrad' type=ReluGrad>]
[STR DEBUG] Add swap-in op at name: "training/RMSprop/gradients/gradients/block2_pool/MaxPool_grad/MaxPoolGrad"
op: "MaxPoolGrad"
input: "block2_conv2/Relu"
input: "block2_pool/MaxPool"
input: "training/RMSprop/gradients/gradients/block3_conv1/Conv2D_grad/Conv2DBackpropInput"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
attr {
  key: "data_format"
  value {
    s: "NHWC"
  }
}
attr {
  key: "ksize"
  value {
    list {
      i: 1
      i: 2
      i: 2
      i: 1
    }
  }
}
attr {
  key: "padding"
  value {
    s: "VALID"
  }
}
attr {
  key: "strides"
  value {
    list {
      i: 1
      i: 2
      i: 2
      i: 1
    }
  }
}
 for consumer [<tf.Operation 'training/RMSprop/gradients/gradients/block2_conv2/Conv2D_grad/Conv2DBackpropFilter' type=Conv2DBackpropFilter>, <tf.Operation 'training/RMSprop/gradients/gradients/block2_conv1/Relu_grad/ReluGrad' type=ReluGrad>]
[STR DEBUG] Processing layer 6's swapping, swap out at [7], swap in at [37]
[STR DEBUG] Find swapout ops: name: "block2_pool/MaxPool"
op: "MaxPool"
input: "block2_conv2/Relu"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
attr {
  key: "data_format"
  value {
    s: "NHWC"
  }
}
attr {
  key: "ksize"
  value {
    list {
      i: 1
      i: 2
      i: 2
      i: 1
    }
  }
}
attr {
  key: "padding"
  value {
    s: "VALID"
  }
}
attr {
  key: "strides"
  value {
    list {
      i: 1
      i: 2
      i: 2
      i: 1
    }
  }
}
, 
	choose swap tensor Tensor("block2_pool/MaxPool:0", shape=(?, 56, 56, 128), dtype=float32), 
	finish at the end of ops: [<tf.Operation 'block3_conv1/Relu' type=Relu>]
[STR DEBUG] Add swap-in op at name: "training/RMSprop/gradients/gradients/block3_conv1/Relu_grad/ReluGrad"
op: "ReluGrad"
input: "training/RMSprop/gradients/gradients/block3_conv2/Conv2D_grad/Conv2DBackpropInput"
input: "block3_conv1/Relu"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
 for consumer [<tf.Operation 'training/RMSprop/gradients/gradients/block2_pool/MaxPool_grad/MaxPoolGrad' type=MaxPoolGrad>, <tf.Operation 'training/RMSprop/gradients/gradients/block3_conv1/Conv2D_grad/Conv2DBackpropFilter' type=Conv2DBackpropFilter>]
[STR DEBUG] Processing layer 7's swapping, swap out at [8], swap in at [36]
[STR DEBUG] Find swapout ops: name: "block3_conv1/Relu"
op: "Relu"
input: "block3_conv1/BiasAdd"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
, 
	choose swap tensor Tensor("block3_conv1/Relu:0", shape=(?, 56, 56, 256), dtype=float32), 
	finish at the end of ops: [<tf.Operation 'block3_conv2/Relu' type=Relu>]
[STR DEBUG] Add swap-in op at name: "training/RMSprop/gradients/gradients/block3_conv2/Relu_grad/ReluGrad"
op: "ReluGrad"
input: "training/RMSprop/gradients/gradients/block3_conv3/Conv2D_grad/Conv2DBackpropInput"
input: "block3_conv2/Relu"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
 for consumer [<tf.Operation 'training/RMSprop/gradients/gradients/block3_conv1/Relu_grad/ReluGrad' type=ReluGrad>, <tf.Operation 'training/RMSprop/gradients/gradients/block3_conv2/Conv2D_grad/Conv2DBackpropFilter' type=Conv2DBackpropFilter>]
[STR DEBUG] Processing layer 8's swapping, swap out at [10], swap in at [14, 34]
[STR DEBUG] Find swapout ops: name: "block3_conv2/Relu"
op: "Relu"
input: "block3_conv2/BiasAdd"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
, 
	choose swap tensor Tensor("block3_conv2/Relu:0", shape=(?, 56, 56, 256), dtype=float32), 
	finish at the end of ops: [<tf.Operation 'block3_pool/MaxPool' type=MaxPool>]
[STR DEBUG] Add swap-in op at name: "block4_pool/MaxPool"
op: "MaxPool"
input: "block4_conv3/Relu"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
attr {
  key: "data_format"
  value {
    s: "NHWC"
  }
}
attr {
  key: "ksize"
  value {
    list {
      i: 1
      i: 2
      i: 2
      i: 1
    }
  }
}
attr {
  key: "padding"
  value {
    s: "VALID"
  }
}
attr {
  key: "strides"
  value {
    list {
      i: 1
      i: 2
      i: 2
      i: 1
    }
  }
}
 for consumer [<tf.Operation 'training/RMSprop/gradients/gradients/block3_conv3/Conv2D_grad/Conv2DBackpropFilter' type=Conv2DBackpropFilter>, <tf.Operation 'training/RMSprop/gradients/gradients/block3_conv2/Relu_grad/ReluGrad' type=ReluGrad>]
[STR DEBUG] Add swap-in op at name: "training/RMSprop/gradients/gradients/block3_pool/MaxPool_grad/MaxPoolGrad"
op: "MaxPoolGrad"
input: "block3_conv3/Relu"
input: "block3_pool/MaxPool"
input: "training/RMSprop/gradients/gradients/block4_conv1/Conv2D_grad/Conv2DBackpropInput"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
attr {
  key: "data_format"
  value {
    s: "NHWC"
  }
}
attr {
  key: "ksize"
  value {
    list {
      i: 1
      i: 2
      i: 2
      i: 1
    }
  }
}
attr {
  key: "padding"
  value {
    s: "VALID"
  }
}
attr {
  key: "strides"
  value {
    list {
      i: 1
      i: 2
      i: 2
      i: 1
    }
  }
}
 for consumer [<tf.Operation 'training/RMSprop/gradients/gradients/block3_conv3/Conv2D_grad/Conv2DBackpropFilter' type=Conv2DBackpropFilter>, <tf.Operation 'training/RMSprop/gradients/gradients/block3_conv2/Relu_grad/ReluGrad' type=ReluGrad>]
[STR DEBUG] Processing layer 10's swapping, swap out at [11], swap in at [33]
[STR DEBUG] Find swapout ops: name: "block3_pool/MaxPool"
op: "MaxPool"
input: "block3_conv3/Relu"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
attr {
  key: "data_format"
  value {
    s: "NHWC"
  }
}
attr {
  key: "ksize"
  value {
    list {
      i: 1
      i: 2
      i: 2
      i: 1
    }
  }
}
attr {
  key: "padding"
  value {
    s: "VALID"
  }
}
attr {
  key: "strides"
  value {
    list {
      i: 1
      i: 2
      i: 2
      i: 1
    }
  }
}
, 
	choose swap tensor Tensor("block3_pool/MaxPool:0", shape=(?, 28, 28, 256), dtype=float32), 
	finish at the end of ops: [<tf.Operation 'block4_conv1/Relu' type=Relu>]
[STR DEBUG] Add swap-in op at name: "training/RMSprop/gradients/gradients/block4_conv1/Relu_grad/ReluGrad"
op: "ReluGrad"
input: "training/RMSprop/gradients/gradients/block4_conv2/Conv2D_grad/Conv2DBackpropInput"
input: "block4_conv1/Relu"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
 for consumer [<tf.Operation 'training/RMSprop/gradients/gradients/block3_pool/MaxPool_grad/MaxPoolGrad' type=MaxPoolGrad>, <tf.Operation 'training/RMSprop/gradients/gradients/block4_conv1/Conv2D_grad/Conv2DBackpropFilter' type=Conv2DBackpropFilter>]
[STR DEBUG] Processing layer 11's swapping, swap out at [12], swap in at [32]
[STR DEBUG] Find swapout ops: name: "block4_conv1/Relu"
op: "Relu"
input: "block4_conv1/BiasAdd"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
, 
	choose swap tensor Tensor("block4_conv1/Relu:0", shape=(?, 28, 28, 512), dtype=float32), 
	finish at the end of ops: [<tf.Operation 'block4_conv2/Relu' type=Relu>]
[STR DEBUG] Add swap-in op at name: "training/RMSprop/gradients/gradients/block4_conv2/Relu_grad/ReluGrad"
op: "ReluGrad"
input: "training/RMSprop/gradients/gradients/block4_conv3/Conv2D_grad/Conv2DBackpropInput"
input: "block4_conv2/Relu"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
 for consumer [<tf.Operation 'training/RMSprop/gradients/gradients/block4_conv1/Relu_grad/ReluGrad' type=ReluGrad>, <tf.Operation 'training/RMSprop/gradients/gradients/block4_conv2/Conv2D_grad/Conv2DBackpropFilter' type=Conv2DBackpropFilter>]
[STR DEBUG] Processing layer 12's swapping, swap out at [14], swap in at [30]
[STR DEBUG] Find swapout ops: name: "block4_conv2/Relu"
op: "Relu"
input: "block4_conv2/BiasAdd"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
, 
	choose swap tensor Tensor("block4_conv2/Relu:0", shape=(?, 28, 28, 512), dtype=float32), 
	finish at the end of ops: [<tf.Operation 'block4_pool/MaxPool' type=MaxPool>]
[STR DEBUG] Add swap-in op at name: "training/RMSprop/gradients/gradients/block4_pool/MaxPool_grad/MaxPoolGrad"
op: "MaxPoolGrad"
input: "block4_conv3/Relu"
input: "block4_pool/MaxPool"
input: "training/RMSprop/gradients/gradients/block5_conv1/Conv2D_grad/Conv2DBackpropInput"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
attr {
  key: "data_format"
  value {
    s: "NHWC"
  }
}
attr {
  key: "ksize"
  value {
    list {
      i: 1
      i: 2
      i: 2
      i: 1
    }
  }
}
attr {
  key: "padding"
  value {
    s: "VALID"
  }
}
attr {
  key: "strides"
  value {
    list {
      i: 1
      i: 2
      i: 2
      i: 1
    }
  }
}
 for consumer [<tf.Operation 'training/RMSprop/gradients/gradients/block4_conv3/Conv2D_grad/Conv2DBackpropFilter' type=Conv2DBackpropFilter>, <tf.Operation 'training/RMSprop/gradients/gradients/block4_conv2/Relu_grad/ReluGrad' type=ReluGrad>]
[STR DEBUG] Processing layer 14's swapping, swap out at [15], swap in at [29]
[STR DEBUG] Find swapout ops: name: "block4_pool/MaxPool"
op: "MaxPool"
input: "block4_conv3/Relu"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
attr {
  key: "data_format"
  value {
    s: "NHWC"
  }
}
attr {
  key: "ksize"
  value {
    list {
      i: 1
      i: 2
      i: 2
      i: 1
    }
  }
}
attr {
  key: "padding"
  value {
    s: "VALID"
  }
}
attr {
  key: "strides"
  value {
    list {
      i: 1
      i: 2
      i: 2
      i: 1
    }
  }
}
, 
	choose swap tensor Tensor("block4_pool/MaxPool:0", shape=(?, 14, 14, 512), dtype=float32), 
	finish at the end of ops: [<tf.Operation 'block5_conv1/Relu' type=Relu>]
[STR DEBUG] Add swap-in op at name: "training/RMSprop/gradients/gradients/block5_conv1/Relu_grad/ReluGrad"
op: "ReluGrad"
input: "training/RMSprop/gradients/gradients/block5_conv2/Conv2D_grad/Conv2DBackpropInput"
input: "block5_conv1/Relu"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
 for consumer [<tf.Operation 'training/RMSprop/gradients/gradients/block4_pool/MaxPool_grad/MaxPoolGrad' type=MaxPoolGrad>, <tf.Operation 'training/RMSprop/gradients/gradients/block5_conv1/Conv2D_grad/Conv2DBackpropFilter' type=Conv2DBackpropFilter>]
[STR DEBUG] Processing layer 15's swapping, swap out at [16], swap in at [28]
[STR DEBUG] Find swapout ops: name: "block5_conv1/Relu"
op: "Relu"
input: "block5_conv1/BiasAdd"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
, 
	choose swap tensor Tensor("block5_conv1/Relu:0", shape=(?, 14, 14, 512), dtype=float32), 
	finish at the end of ops: [<tf.Operation 'block5_conv2/Relu' type=Relu>]
[STR DEBUG] Add swap-in op at name: "training/RMSprop/gradients/gradients/block5_conv2/Relu_grad/ReluGrad"
op: "ReluGrad"
input: "training/RMSprop/gradients/gradients/block5_conv3/Conv2D_grad/Conv2DBackpropInput"
input: "block5_conv2/Relu"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
 for consumer [<tf.Operation 'training/RMSprop/gradients/gradients/block5_conv1/Relu_grad/ReluGrad' type=ReluGrad>, <tf.Operation 'training/RMSprop/gradients/gradients/block5_conv2/Conv2D_grad/Conv2DBackpropFilter' type=Conv2DBackpropFilter>]
[STR DEBUG] Processing layer 16's swapping, swap out at [19], swap in at [25]
[STR DEBUG] Cannot find swap-out control nodes for layer 16, use the first op name: "flatten/Shape"
op: "Shape"
input: "block5_pool/MaxPool"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
attr {
  key: "out_type"
  value {
    type: DT_INT32
  }
}

[STR DEBUG] Find swapout ops: name: "block5_conv2/Relu"
op: "Relu"
input: "block5_conv2/BiasAdd"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
, 
	choose swap tensor Tensor("block5_conv2/Relu:0", shape=(?, 14, 14, 512), dtype=float32), 
	finish at the end of ops: [<tf.Operation 'flatten/Shape' type=Shape>]
[STR DEBUG] Cannot find swap-in control nodes for ref. layer 25, use the last op name: "training/RMSprop/gradients/gradients/flatten/Reshape_grad/Reshape"
op: "Reshape"
input: "training/RMSprop/gradients/gradients/fc1/MatMul_grad/MatMul"
input: "training/RMSprop/gradients/gradients/flatten/Reshape_grad/Shape"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
attr {
  key: "Tshape"
  value {
    type: DT_INT32
  }
}

[STR DEBUG] Add swap-in op at name: "training/RMSprop/gradients/gradients/flatten/Reshape_grad/Reshape"
op: "Reshape"
input: "training/RMSprop/gradients/gradients/fc1/MatMul_grad/MatMul"
input: "training/RMSprop/gradients/gradients/flatten/Reshape_grad/Shape"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
attr {
  key: "Tshape"
  value {
    type: DT_INT32
  }
}
 for consumer [<tf.Operation 'training/RMSprop/gradients/gradients/block5_conv3/Conv2D_grad/Conv2DBackpropFilter' type=Conv2DBackpropFilter>, <tf.Operation 'training/RMSprop/gradients/gradients/block5_conv2/Relu_grad/ReluGrad' type=ReluGrad>]
[STR DEBUG] Processing layer 19's swapping, swap out at [20], swap in at [24]
[STR DEBUG] Cannot find swap-out activation nodes for layer 19, among [<tf.Operation 'flatten/Shape' type=Shape>, <tf.Operation 'flatten/strided_slice/stack_2' type=Const>, <tf.Operation 'flatten/strided_slice' type=StridedSlice>, <tf.Operation 'flatten/Reshape/shape/1' type=Const>, <tf.Operation 'flatten/Reshape' type=Reshape>, <tf.Operation 'flatten/Reshape/shape' type=Pack>, <tf.Operation 'flatten/strided_slice/stack' type=Const>, <tf.Operation 'flatten/strided_slice/stack_1' type=Const>]
[STR DEBUG] Choose op that have outputs: name: "flatten/strided_slice/stack_1"
op: "Const"
attr {
  key: "dtype"
  value {
    type: DT_INT32
  }
}
attr {
  key: "value"
  value {
    tensor {
      dtype: DT_INT32
      tensor_shape {
        dim {
          size: 1
        }
      }
      int_val: 1
    }
  }
}

[STR DEBUG] Find swapout ops: name: "flatten/strided_slice/stack_1"
op: "Const"
attr {
  key: "dtype"
  value {
    type: DT_INT32
  }
}
attr {
  key: "value"
  value {
    tensor {
      dtype: DT_INT32
      tensor_shape {
        dim {
          size: 1
        }
      }
      int_val: 1
    }
  }
}
, 
	choose swap tensor Tensor("flatten/strided_slice/stack_1:0", shape=(1,), dtype=int32), 
	finish at the end of ops: [<tf.Operation 'fc1/Relu' type=Relu>]
[STR DEBUG] Processing layer 20's swapping, swap out at [21], swap in at [23]
[STR DEBUG] Find swapout ops: name: "fc1/Relu"
op: "Relu"
input: "fc1/BiasAdd"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
, 
	choose swap tensor Tensor("fc1/Relu:0", shape=(?, 4096), dtype=float32), 
	finish at the end of ops: [<tf.Operation 'fc2/Relu' type=Relu>]
[STR DEBUG] Add swap-in op at name: "training/RMSprop/gradients/gradients/fc2/Relu_grad/ReluGrad"
op: "ReluGrad"
input: "training/RMSprop/gradients/gradients/predictions/MatMul_grad/MatMul"
input: "fc2/Relu"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
 for consumer [<tf.Operation 'training/RMSprop/gradients/gradients/fc2/MatMul_grad/MatMul_1' type=MatMul>, <tf.Operation 'training/RMSprop/gradients/gradients/fc1/Relu_grad/ReluGrad' type=ReluGrad>]
[STR DEBUG] Cannot find swap-out operations for swap-in at layer 21, skipping
  1/182 [..............................] - ETA: 1:07:28 - loss: 2.3515 - acc: 0.1045  2/182 [..............................] - ETA: 35:40 - loss: 1654996.8008 - acc: 0.1068  3/182 [..............................] - ETA: 25:00 - loss: 1103345.0535 - acc: 0.0985  4/182 [..............................] - ETA: 19:40 - loss: 827514.6489 - acc: 0.0989   5/182 [..............................] - ETA: 16:27 - loss: 662439.5809 - acc: 0.0991  6/182 [..............................] - ETA: 14:17 - loss: 552033.4886 - acc: 0.1000  7/182 [>.............................] - ETA: 12:44 - loss: 473173.7067 - acc: 0.0974  8/182 [>.............................] - ETA: 11:34 - loss: 414027.2888 - acc: 0.0994  9/182 [>.............................] - ETA: 10:40 - loss: 368024.5163 - acc: 0.1005 10/182 [>.............................] - ETA: 9:57 - loss: 331222.2963 - acc: 0.1014  11/182 [>.............................] - ETA: 9:21 - loss: 301111.3879 - acc: 0.1017 12/182 [>.............................] - ETA: 8:52 - loss: 276018.9641 - acc: 0.1008 13/182 [=>............................] - ETA: 8:25 - loss: 254786.9128 - acc: 0.1014 14/182 [=>............................] - ETA: 8:02 - loss: 236588.0116 - acc: 0.1000 15/182 [=>............................] - ETA: 7:43 - loss: 220815.6310 - acc: 0.1033 16/182 [=>............................] - ETA: 7:27 - loss: 207014.7999 - acc: 0.1011 17/182 [=>............................] - ETA: 7:13 - loss: 194837.5935 - acc: 0.1021 18/182 [=>............................] - ETA: 7:00 - loss: 184013.4180 - acc: 0.1020 19/182 [==>...........................] - ETA: 6:48 - loss: 174328.6226 - acc: 0.1012 20/182 [==>...........................] - ETA: 6:37 - loss: 165612.3065 - acc: 0.1011 21/182 [==>...........................] - ETA: 6:28 - loss: 157726.1156 - acc: 0.1013 22/182 [==>...........................] - ETA: 6:19 - loss: 150556.8513 - acc: 0.1037 23/182 [==>...........................] - ETA: 6:10 - loss: 144011.0013 - acc: 0.1042 24/182 [==>...........................] - ETA: 6:01 - loss: 138010.6386 - acc: 0.1049 25/182 [===>..........................] - ETA: 5:54 - loss: 132490.3046 - acc: 0.1062 26/182 [===>..........................] - ETA: 5:47 - loss: 127394.6092 - acc: 0.1075 27/182 [===>..........................] - ETA: 5:40 - loss: 122676.4058 - acc: 0.1082 28/182 [===>..........................] - ETA: 5:35 - loss: 118295.1969 - acc: 0.1073 29/182 [===>..........................] - ETA: 5:29 - loss: 114216.1316 - acc: 0.1071 30/182 [===>..........................] - ETA: 5:24 - loss: 110409.0039 - acc: 0.1062 31/182 [====>.........................] - ETA: 5:19 - loss: 106847.4973 - acc: 0.1070 32/182 [====>.........................] - ETA: 5:14 - loss: 103508.5848 - acc: 0.1067 33/182 [====>.........................] - ETA: 5:09 - loss: 100372.0312 - acc: 0.1062 34/182 [====>.........................] - ETA: 5:05 - loss: 97419.9802 - acc: 0.1068  35/182 [====>.........................] - ETA: 5:01 - loss: 94636.6177 - acc: 0.1088 36/182 [====>.........................] - ETA: 4:56 - loss: 92007.8855 - acc: 0.1095 37/182 [=====>........................] - ETA: 4:52 - loss: 89521.2478 - acc: 0.1115 38/182 [=====>........................] - ETA: 4:48 - loss: 87165.5760 - acc: 0.1117 39/182 [=====>........................] - ETA: 4:44 - loss: 84930.6202 - acc: 0.1121 40/182 [=====>........................] - ETA: 4:41 - loss: 82807.4122 - acc: 0.1126 41/182 [=====>........................] - ETA: 4:38 - loss: 80787.7747 - acc: 0.1119 42/182 [=====>........................] - ETA: 4:34 - loss: 78864.3105 - acc: 0.1120 43/182 [======>.......................] - ETA: 4:31 - loss: 77030.3104 - acc: 0.1111 44/182 [======>.......................] - ETA: 4:28 - loss: 75279.6739 - acc: 0.1112 45/182 [======>.......................] - ETA: 4:25 - loss: 73606.8434 - acc: 0.1106 46/182 [======>.......................] - ETA: 4:22 - loss: 72006.7448 - acc: 0.1104 47/182 [======>.......................] - ETA: 4:19 - loss: 70474.7354 - acc: 0.1100 48/182 [======>.......................] - ETA: 4:16 - loss: 69006.5597 - acc: 0.1096 49/182 [=======>......................] - ETA: 4:13 - loss: 67598.3096 - acc: 0.1092 50/182 [=======>......................] - ETA: 4:10 - loss: 66246.3895 - acc: 0.1086 51/182 [=======>......................] - ETA: 4:07 - loss: 64947.4858 - acc: 0.1084 52/182 [=======>......................] - ETA: 4:05 - loss: 63698.5400 - acc: 0.1080 53/182 [=======>......................] - ETA: 4:02 - loss: 62496.7242 - acc: 0.1082 54/182 [=======>......................] - ETA: 3:59 - loss: 61339.4201 - acc: 0.1082 55/182 [========>.....................] - ETA: 3:57 - loss: 60224.1997 - acc: 0.1075 56/182 [========>.....................] - ETA: 3:54 - loss: 59148.8087 - acc: 0.1069 57/182 [========>.....................] - ETA: 3:52 - loss: 58111.1507 - acc: 0.1068 58/182 [========>.....................] - ETA: 3:49 - loss: 57109.2740 - acc: 0.1071 59/182 [========>.....................] - ETA: 3:47 - loss: 56141.3592 - acc: 0.1071 60/182 [========>.....................] - ETA: 3:44 - loss: 55205.7083 - acc: 0.1072 61/182 [=========>....................] - ETA: 3:42 - loss: 54300.7343 - acc: 0.1074 62/182 [=========>....................] - ETA: 3:40 - loss: 53424.9531 - acc: 0.1073 63/182 [=========>....................] - ETA: 3:37 - loss: 52576.9743 - acc: 0.1074 64/182 [=========>....................] - ETA: 3:35 - loss: 51755.4950 - acc: 0.1067 65/182 [=========>....................] - ETA: 3:33 - loss: 50959.2912 - acc: 0.1067 66/182 [=========>....................] - ETA: 3:31 - loss: 50187.2143 - acc: 0.1074 67/182 [==========>...................] - ETA: 3:28 - loss: 49438.2243 - acc: 0.1076 68/182 [==========>...................] - ETA: 3:26 - loss: 48711.2255 - acc: 0.1076 69/182 [==========>...................] - ETA: 3:24 - loss: 48005.3044 - acc: 0.1072 70/182 [==========>...................] - ETA: 3:22 - loss: 47319.5471 - acc: 0.1069 71/182 [==========>...................] - ETA: 3:20 - loss: 46653.1071 - acc: 0.1069 72/182 [==========>...................] - ETA: 3:17 - loss: 46005.1793 - acc: 0.1069 73/182 [===========>..................] - ETA: 3:15 - loss: 45375.0028 - acc: 0.1068 74/182 [===========>..................] - ETA: 3:12 - loss: 44761.8582 - acc: 0.1068 75/182 [===========>..................] - ETA: 3:10 - loss: 44165.0641 - acc: 0.1064 76/182 [===========>..................] - ETA: 3:08 - loss: 43583.9752 - acc: 0.1069 77/182 [===========>..................] - ETA: 3:06 - loss: 43017.9794 - acc: 0.1068 78/182 [===========>..................] - ETA: 3:04 - loss: 42466.4964 - acc: 0.1069 79/182 [============>.................] - ETA: 3:02 - loss: 41928.9749 - acc: 0.1069 80/182 [============>.................] - ETA: 3:00 - loss: 41404.8915 - acc: 0.1072 81/182 [============>.................] - ETA: 2:58 - loss: 40893.7483 - acc: 0.1074 82/182 [============>.................] - ETA: 2:56 - loss: 40395.0721 - acc: 0.1073 83/182 [============>.................] - ETA: 2:54 - loss: 39908.4126 - acc: 0.1072 84/182 [============>.................] - ETA: 2:52 - loss: 39433.3399 - acc: 0.1070 85/182 [=============>................] - ETA: 2:50 - loss: 38969.4453 - acc: 0.1066 86/182 [=============>................] - ETA: 2:48 - loss: 38516.3443 - acc: 0.1066 87/182 [=============>................] - ETA: 2:46 - loss: 38073.6542 - acc: 0.1063 88/182 [=============>................] - ETA: 2:44 - loss: 37641.0252 - acc: 0.1061 89/182 [=============>................] - ETA: 2:42 - loss: 37218.1182 - acc: 0.1062 90/182 [=============>................] - ETA: 2:40 - loss: 36804.6092 - acc: 0.1058 91/182 [==============>...............] - ETA: 2:38 - loss: 36400.1882 - acc: 0.1060 92/182 [==============>...............] - ETA: 2:36 - loss: 36004.5590 - acc: 0.1060 93/182 [==============>...............] - ETA: 2:34 - loss: 35617.4380 - acc: 0.1056 94/182 [==============>...............] - ETA: 2:32 - loss: 35238.5536 - acc: 0.1056 95/182 [==============>...............] - ETA: 2:30 - loss: 34867.6453 - acc: 0.1056 96/182 [==============>...............] - ETA: 2:28 - loss: 34504.4965 - acc: 0.1058 97/182 [==============>...............] - ETA: 2:27 - loss: 34148.8038 - acc: 0.1058 98/182 [===============>..............] - ETA: 2:25 - loss: 33800.3701 - acc: 0.10582022-01-08 22:45:51.116442: W tensorflow/core/common_runtime/bfc_allocator.cc:419] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.63GiB (rounded to 2825912320).  Current allocation summary follows.
2022-01-08 22:45:51.117479: W tensorflow/core/common_runtime/bfc_allocator.cc:424] ************___________*************_*__*************_______***______________*__________****___*****
2022-01-08 22:45:51.117547: W tensorflow/core/framework/op_kernel.cc:1651] OP_REQUIRES failed at conv_grad_input_ops.cc:1063 : Resource exhausted: OOM when allocating tensor with shape[220,64,224,224] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
Traceback (most recent call last):
  File "../run_training.py", line 83, in <module>
    verbose=1)
  File "/home/zongzan/.conda/envs/STR36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py", line 728, in fit
    use_multiprocessing=use_multiprocessing)
  File "/home/zongzan/.conda/envs/STR36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_generator.py", line 603, in fit
    steps_name='steps_per_epoch')
  File "/home/zongzan/.conda/envs/STR36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_generator.py", line 265, in model_iteration
    batch_outs = batch_function(*batch_data)
  File "/home/zongzan/.conda/envs/STR36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py", line 1021, in train_on_batch
    outputs = self.train_function(ins)  # pylint: disable=not-callable
  File "/home/zongzan/.conda/envs/STR36/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py", line 3476, in __call__
    run_metadata=self.run_metadata)
  File "/home/zongzan/.conda/envs/STR36/lib/python3.6/site-packages/tensorflow_core/python/client/session.py", line 1472, in __call__
    run_metadata_ptr)
tensorflow.python.framework.errors_impl.ResourceExhaustedError: 2 root error(s) found.
  (0) Resource exhausted: OOM when allocating tensor with shape[220,64,224,224] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
	 [[{{node training/RMSprop/gradients/gradients/block1_conv2/Conv2D_grad/Conv2DBackpropInput}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

	 [[training/RMSprop/gradients/gradients/block1_conv1/Relu_grad/ReluGrad/_164]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

  (1) Resource exhausted: OOM when allocating tensor with shape[220,64,224,224] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
	 [[{{node training/RMSprop/gradients/gradients/block1_conv2/Conv2D_grad/Conv2DBackpropInput}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

0 successful operations.
0 derived errors ignored.

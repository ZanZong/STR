WARNING:tensorflow:From /home/zongzan/.conda/envs/STR36/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
1 Physical GPUs, 1 Logical GPUs
Parsing arguments: [('batch_size', 50), ('epochs', 1), ('model_name', 'vgg_unet'), ('strategy', 'capuchin'), ('verbose', False)]
Cannot find config for batch_size=50, use 40 instead
There are 367 and 367 images downloaded for train and test
Verifying train dataset
  0%|          | 0/367 [00:00<?, ?it/s]  2%|▏         | 8/367 [00:00<00:04, 74.17it/s]  5%|▌         | 20/367 [00:00<00:03, 98.61it/s]  9%|▉         | 33/367 [00:00<00:03, 111.30it/s] 13%|█▎        | 47/367 [00:00<00:02, 119.61it/s] 16%|█▋        | 60/367 [00:00<00:02, 122.59it/s] 20%|█▉        | 73/367 [00:00<00:02, 124.79it/s] 23%|██▎       | 86/367 [00:00<00:02, 125.91it/s] 27%|██▋       | 100/367 [00:00<00:02, 128.23it/s] 32%|███▏      | 116/367 [00:00<00:01, 136.87it/s] 36%|███▌      | 132/367 [00:01<00:01, 143.00it/s] 41%|████      | 149/367 [00:01<00:01, 150.94it/s] 46%|████▌     | 167/367 [00:01<00:01, 157.09it/s] 50%|█████     | 185/367 [00:01<00:01, 161.86it/s] 55%|█████▌    | 202/367 [00:01<00:01, 164.17it/s] 60%|█████▉    | 220/367 [00:01<00:00, 165.96it/s] 65%|██████▍   | 237/367 [00:01<00:00, 166.69it/s] 69%|██████▉   | 255/367 [00:01<00:00, 168.88it/s] 74%|███████▍  | 273/367 [00:01<00:00, 169.18it/s] 79%|███████▉  | 291/367 [00:01<00:00, 170.84it/s] 84%|████████▍ | 309/367 [00:02<00:00, 169.82it/s] 89%|████████▉ | 327/367 [00:02<00:00, 171.77it/s] 94%|█████████▍| 345/367 [00:02<00:00, 170.47it/s] 99%|█████████▉| 363/367 [00:02<00:00, 170.13it/s]100%|██████████| 367/367 [00:02<00:00, 152.32it/s]
WARNING:tensorflow:
The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
  * https://github.com/tensorflow/io (for I/O related ops)
If you depend on functionality not listed there, please file an issue.

WARNING:tensorflow:From /home/zongzan/.conda/envs/STR36/lib/python3.6/site-packages/tensorflow_core/python/ops/stropt/lms.py:970: The name tf.train.SessionRunHook is deprecated. Please use tf.estimator.SessionRunHook instead.

WARNING:tensorflow:From /home/zongzan/.conda/envs/STR36/lib/python3.6/site-packages/tensorflow_core/python/ops/stropt/str_strategy.py:122: get_backward_walk_ops (from tensorflow.contrib.graph_editor.select) is deprecated and will be removed after 2019-06-06.
Instructions for updating:
Please use tensorflow.python.ops.op_selector.get_backward_walk_ops.
WARNING:tensorflow:From /home/zongzan/.conda/envs/STR36/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:From /home/zongzan/.conda/envs/STR36/lib/python3.6/site-packages/tensorflow_core/python/ops/stropt/lms.py:928: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.

Dataset verified! 
Starting Epoch  0
[STR DEBUG] Parsing STR configuration: [{'strategy': 'hybrid', 'verbose': 'false', 'tags': '/home/zongzan/dist_dnn_training/STR/optimizer/logs/vgg_unet_40/layer_names'}, {'r': '/home/zongzan/dist_dnn_training/STR/optimizer/logs/vgg_unet_40/R-capuchin', 'p': '/home/zongzan/dist_dnn_training/STR/optimizer/logs/vgg_unet_40/P-capuchin', 'q': '/home/zongzan/dist_dnn_training/STR/optimizer/logs/vgg_unet_40/Q-capuchin'}]
[STR DEBUG] Processing layer 28's swapping, swap out at [30], swap in at [41]
[STR DEBUG] Cannot find swap-out activation nodes for layer 28, among [<tf.Operation 'up_sampling2d_2/Shape' type=Shape>, <tf.Operation 'up_sampling2d_2/Const' type=Const>, <tf.Operation 'up_sampling2d_2/strided_slice/stack_1' type=Const>, <tf.Operation 'up_sampling2d_2/strided_slice/stack' type=Const>, <tf.Operation 'up_sampling2d_2/strided_slice/stack_2' type=Const>, <tf.Operation 'up_sampling2d_2/resize/ResizeNearestNeighbor' type=ResizeNearestNeighbor>, <tf.Operation 'up_sampling2d_2/strided_slice' type=StridedSlice>, <tf.Operation 'up_sampling2d_2/mul' type=Mul>]
[STR DEBUG] Choose op that have outputs: name: "up_sampling2d_2/mul"
op: "Mul"
input: "up_sampling2d_2/strided_slice"
input: "up_sampling2d_2/Const"
attr {
  key: "T"
  value {
    type: DT_INT32
  }
}

[STR DEBUG] Cannot find swap-out control nodes for layer 28, use the first op name: "zero_padding2d_3/Pad/paddings"
op: "Const"
attr {
  key: "dtype"
  value {
    type: DT_INT32
  }
}
attr {
  key: "value"
  value {
    tensor {
      dtype: DT_INT32
      tensor_shape {
        dim {
          size: 4
        }
        dim {
          size: 2
        }
      }
      tensor_content: "\000\000\000\000\000\000\000\000\001\000\000\000\001\000\000\000\001\000\000\000\001\000\000\000\000\000\000\000\000\000\000\000"
    }
  }
}

[STR DEBUG] Find swapout ops: name: "up_sampling2d_2/mul"
op: "Mul"
input: "up_sampling2d_2/strided_slice"
input: "up_sampling2d_2/Const"
attr {
  key: "T"
  value {
    type: DT_INT32
  }
}
, 
	choose swap tensor Tensor("up_sampling2d_2/mul:0", shape=(2,), dtype=int32), 
	finish at the end of ops: [<tf.Operation 'zero_padding2d_3/Pad/paddings' type=Const>]
[STR DEBUG] Cannot find swap-in control nodes for ref. layer 41, use the last op name: "training/Adadelta/gradients/gradients/training/Adadelta/gradients/concatenate_2/concat_grad/Slice_1"
op: "Slice"
input: "training/Adadelta/gradients/gradients/training/Adadelta/gradients/zero_padding2d_3/Pad_grad/Slice_1"
input: "training/Adadelta/gradients/gradients/training/Adadelta/gradients/concatenate_2/concat_grad/ConcatOffset:1"
input: "training/Adadelta/gradients/gradients/training/Adadelta/gradients/concatenate_2/concat_grad/ShapeN:1"
attr {
  key: "Index"
  value {
    type: DT_INT32
  }
}
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}

[STR DEBUG] Processing layer 29's swapping, swap out at [31], swap in at [40]
[STR DEBUG] Cannot find swap-out activation nodes for layer 29, among [<tf.Operation 'concatenate_2/concat/axis' type=Const>, <tf.Operation 'concatenate_2/concat' type=ConcatV2>]
[STR DEBUG] Choose op that have outputs: name: "concatenate_2/concat"
op: "ConcatV2"
input: "up_sampling2d_2/resize/ResizeNearestNeighbor"
input: "block1_pool/MaxPool"
input: "concatenate_2/concat/axis"
attr {
  key: "N"
  value {
    i: 2
  }
}
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
attr {
  key: "Tidx"
  value {
    type: DT_INT32
  }
}

[STR DEBUG] Cannot find swap-out control nodes for layer 29, use the first op name: "conv2d_3/kernel"
op: "VarHandleOp"
attr {
  key: "_class"
  value {
    list {
      s: "loc:@conv2d_3/kernel"
    }
  }
}
attr {
  key: "container"
  value {
    s: ""
  }
}
attr {
  key: "dtype"
  value {
    type: DT_FLOAT
  }
}
attr {
  key: "shape"
  value {
    shape {
      dim {
        size: 3
      }
      dim {
        size: 3
      }
      dim {
        size: 192
      }
      dim {
        size: 64
      }
    }
  }
}
attr {
  key: "shared_name"
  value {
    s: "conv2d_3/kernel"
  }
}

[STR DEBUG] Find swapout ops: name: "concatenate_2/concat"
op: "ConcatV2"
input: "up_sampling2d_2/resize/ResizeNearestNeighbor"
input: "block1_pool/MaxPool"
input: "concatenate_2/concat/axis"
attr {
  key: "N"
  value {
    i: 2
  }
}
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
attr {
  key: "Tidx"
  value {
    type: DT_INT32
  }
}
, 
	choose swap tensor Tensor("concatenate_2/concat:0", shape=(?, 160, 320, 192), dtype=float32), 
	finish at the end of ops: [<tf.Operation 'conv2d_3/kernel' type=VarHandleOp>]
[STR DEBUG] Cannot find swap-in control nodes for ref. layer 40, use the last op name: "training/Adadelta/gradients/zero_padding2d_3/Pad"
op: "Pad"
input: "training/Adadelta/gradients/concatenate_2/concat"
input: "zero_padding2d_3/Pad/paddings"
input: "^loss/mul"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
attr {
  key: "Tpaddings"
  value {
    type: DT_INT32
  }
}

[STR DEBUG] Processing layer 31's swapping, swap out at [34], swap in at [38]
[STR DEBUG] Cannot find swap-out activation nodes for layer 31, among [<tf.Operation 'conv2d_3/kernel' type=VarHandleOp>, <tf.Operation 'conv2d_3/kernel/IsInitialized/VarIsInitializedOp' type=VarIsInitializedOp>, <tf.Operation 'conv2d_3/kernel/Read/ReadVariableOp' type=ReadVariableOp>, <tf.Operation 'conv2d_3/kernel/Assign' type=AssignVariableOp>, <tf.Operation 'conv2d_3/bias/Initializer/zeros' type=Const>, <tf.Operation 'conv2d_3/bias' type=VarHandleOp>, <tf.Operation 'conv2d_3/bias/IsInitialized/VarIsInitializedOp' type=VarIsInitializedOp>, <tf.Operation 'conv2d_3/bias/Read/ReadVariableOp' type=ReadVariableOp>, <tf.Operation 'conv2d_3/bias/Assign' type=AssignVariableOp>, <tf.Operation 'conv2d_3/Conv2D' type=Conv2D>, <tf.Operation 'conv2d_3/BiasAdd' type=BiasAdd>, <tf.Operation 'conv2d_3/BiasAdd/ReadVariableOp' type=ReadVariableOp>, <tf.Operation 'conv2d_3/Conv2D/ReadVariableOp' type=ReadVariableOp>, <tf.Operation 'conv2d_3/dilation_rate' type=Const>, <tf.Operation 'conv2d_3/kernel/Initializer/random_uniform/shape' type=Const>, <tf.Operation 'conv2d_3/kernel/Initializer/random_uniform/min' type=Const>, <tf.Operation 'conv2d_3/kernel/Initializer/random_uniform/max' type=Const>, <tf.Operation 'conv2d_3/kernel/Initializer/random_uniform/RandomUniform' type=RandomUniform>, <tf.Operation 'conv2d_3/kernel/Initializer/random_uniform/sub' type=Sub>, <tf.Operation 'conv2d_3/kernel/Initializer/random_uniform/mul' type=Mul>, <tf.Operation 'conv2d_3/kernel/Initializer/random_uniform' type=Add>]
[STR DEBUG] Choose op that have outputs: name: "conv2d_3/kernel/Initializer/random_uniform"
op: "Add"
input: "conv2d_3/kernel/Initializer/random_uniform/mul"
input: "conv2d_3/kernel/Initializer/random_uniform/min"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
attr {
  key: "_class"
  value {
    list {
      s: "loc:@conv2d_3/kernel"
    }
  }
}

[STR DEBUG] Cannot find swap-out control nodes for layer 31, use the first op name: "reshape/Reshape/shape"
op: "Pack"
input: "reshape/strided_slice"
input: "reshape/Reshape/shape/1"
input: "reshape/Reshape/shape/2"
attr {
  key: "N"
  value {
    i: 3
  }
}
attr {
  key: "T"
  value {
    type: DT_INT32
  }
}
attr {
  key: "axis"
  value {
    i: 0
  }
}

[STR DEBUG] Find swapout ops: name: "conv2d_3/kernel/Initializer/random_uniform"
op: "Add"
input: "conv2d_3/kernel/Initializer/random_uniform/mul"
input: "conv2d_3/kernel/Initializer/random_uniform/min"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
attr {
  key: "_class"
  value {
    list {
      s: "loc:@conv2d_3/kernel"
    }
  }
}
, 
	choose swap tensor Tensor("conv2d_3/kernel/Initializer/random_uniform:0", shape=(3, 3, 192, 64), dtype=float32), 
	finish at the end of ops: [<tf.Operation 'reshape/Reshape/shape' type=Pack>]
[STR DEBUG] Cannot find swap-in control nodes for ref. layer 38, use the last op name: "training/Adadelta/gradients/batch_normalization_3/cond/FusedBatchNormV3_1"
op: "FusedBatchNormV3"
input: "training/Adadelta/gradients/batch_normalization_3/cond/FusedBatchNormV3_1/Switch:0"
input: "batch_normalization_3/cond/ReadVariableOp_2"
input: "batch_normalization_3/cond/ReadVariableOp_3"
input: "batch_normalization_3/cond/FusedBatchNormV3_1/ReadVariableOp"
input: "batch_normalization_3/cond/FusedBatchNormV3_1/ReadVariableOp_1"
input: "^loss/mul"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
attr {
  key: "U"
  value {
    type: DT_FLOAT
  }
}
attr {
  key: "data_format"
  value {
    s: "NHWC"
  }
}
attr {
  key: "epsilon"
  value {
    f: 0.0010000000474974513
  }
}
attr {
  key: "is_training"
  value {
    b: false
  }
}
2022-01-07 18:49:40.958253: W tensorflow/core/common_runtime/bfc_allocator.cc:419] Allocator (GPU_0_bfc) ran out of memory trying to allocate 488.28MiB (rounded to 512000000).  Current allocation summary follows.
2022-01-07 18:49:40.959203: W tensorflow/core/common_runtime/bfc_allocator.cc:424] ***************************************************_******_*****************************************
2022-01-07 18:49:40.959263: W tensorflow/core/framework/op_kernel.cc:1651] OP_REQUIRES failed at cast_op.cc:109 : Resource exhausted: OOM when allocating tensor with shape[50,51200,50] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
2022-01-07 18:49:40.959310: W tensorflow/core/common_runtime/bfc_allocator.cc:419] Allocator (GPU_0_bfc) ran out of memory trying to allocate 488.28MiB (rounded to 512000000).  Current allocation summary follows.
2022-01-07 18:49:40.960285: W tensorflow/core/common_runtime/bfc_allocator.cc:424] ***************************************************_******_*****************************************
2022-01-07 18:49:40.960347: W tensorflow/core/framework/op_kernel.cc:1651] OP_REQUIRES failed at cwise_ops_common.h:259 : Resource exhausted: OOM when allocating tensor with shape[50,51200,50] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc

[STR DEBUG] Processing layer 33's swapping, swap out at [35], swap in at [36]
[STR DEBUG] Cannot find swap-out activation nodes for layer 33, among [<tf.Operation 'conv2d_4/bias/Initializer/zeros' type=Const>, <tf.Operation 'conv2d_4/bias' type=VarHandleOp>, <tf.Operation 'conv2d_4/bias/IsInitialized/VarIsInitializedOp' type=VarIsInitializedOp>, <tf.Operation 'conv2d_4/Conv2D' type=Conv2D>, <tf.Operation 'conv2d_4/bias/Read/ReadVariableOp' type=ReadVariableOp>, <tf.Operation 'conv2d_4/bias/Assign' type=AssignVariableOp>, <tf.Operation 'conv2d_4/BiasAdd' type=BiasAdd>, <tf.Operation 'conv2d_4/BiasAdd/ReadVariableOp' type=ReadVariableOp>, <tf.Operation 'conv2d_4/Conv2D/ReadVariableOp' type=ReadVariableOp>, <tf.Operation 'conv2d_4/dilation_rate' type=Const>, <tf.Operation 'conv2d_4/kernel/Initializer/random_uniform/shape' type=Const>, <tf.Operation 'conv2d_4/kernel/Initializer/random_uniform/min' type=Const>, <tf.Operation 'conv2d_4/kernel' type=VarHandleOp>, <tf.Operation 'conv2d_4/kernel/Initializer/random_uniform/max' type=Const>, <tf.Operation 'conv2d_4/kernel/Initializer/random_uniform/RandomUniform' type=RandomUniform>, <tf.Operation 'conv2d_4/kernel/Initializer/random_uniform/sub' type=Sub>, <tf.Operation 'conv2d_4/kernel/Initializer/random_uniform' type=Add>, <tf.Operation 'conv2d_4/kernel/Initializer/random_uniform/mul' type=Mul>, <tf.Operation 'conv2d_4/kernel/IsInitialized/VarIsInitializedOp' type=VarIsInitializedOp>, <tf.Operation 'conv2d_4/kernel/Read/ReadVariableOp' type=ReadVariableOp>, <tf.Operation 'conv2d_4/kernel/Assign' type=AssignVariableOp>]
[STR DEBUG] Choose op that have outputs: name: "conv2d_4/kernel/Read/ReadVariableOp"
op: "ReadVariableOp"
input: "conv2d_4/kernel"
attr {
  key: "dtype"
  value {
    type: DT_FLOAT
  }
}

[STR DEBUG] Cannot find swap-out control nodes for layer 33, use the first op name: "loss/activation_loss/value"
op: "DivNoNan"
input: "loss/activation_loss/Sum_3"
input: "loss/activation_loss/num_elements/Cast"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}

[STR DEBUG] Find swapout ops: name: "conv2d_4/kernel/Read/ReadVariableOp"
op: "ReadVariableOp"
input: "conv2d_4/kernel"
attr {
  key: "dtype"
  value {
    type: DT_FLOAT
  }
}
, 
	choose swap tensor Tensor("conv2d_4/kernel/Read/ReadVariableOp:0", shape=(3, 3, 64, 50), dtype=float32), 
	finish at the end of ops: [<tf.Operation 'loss/activation_loss/value' type=DivNoNan>]
[STR DEBUG] Cannot find swap-in control nodes for ref. layer 36, use the last op name: "training/Adadelta/gradients/reshape/Reshape"
op: "Reshape"
input: "training/Adadelta/gradients/conv2d_4/BiasAdd"
input: "training/Adadelta/gradients/reshape/Reshape/shape"
input: "^loss/mul"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
attr {
  key: "Tshape"
  value {
    type: DT_INT32
  }
}

Traceback (most recent call last):
  File "../run_training.py", line 68, in <module>
    epochs=1)
  File "/home/zongzan/.conda/envs/STR36/lib/python3.6/site-packages/keras_segmentation-0.2.0remat-py3.6.egg/keras_segmentation/train.py", line 106, in train
  File "/home/zongzan/.conda/envs/STR36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py", line 1301, in fit_generator
    steps_name='steps_per_epoch')
  File "/home/zongzan/.conda/envs/STR36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_generator.py", line 265, in model_iteration
    batch_outs = batch_function(*batch_data)
  File "/home/zongzan/.conda/envs/STR36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py", line 1021, in train_on_batch
    outputs = self.train_function(ins)  # pylint: disable=not-callable
  File "/home/zongzan/.conda/envs/STR36/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py", line 3476, in __call__
    run_metadata=self.run_metadata)
  File "/home/zongzan/.conda/envs/STR36/lib/python3.6/site-packages/tensorflow_core/python/client/session.py", line 1472, in __call__
    run_metadata_ptr)
tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[50,51200,50] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
	 [[{{node training/Adadelta/gradients/gradients/training/Adadelta/gradients/activation/Max_grad/Cast}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.


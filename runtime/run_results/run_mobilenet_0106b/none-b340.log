WARNING:tensorflow:From /home/zongzan/.conda/envs/STR36/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
WARNING:tensorflow:
The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
  * https://github.com/tensorflow/io (for I/O related ops)
If you depend on functionality not listed there, please file an issue.

WARNING:tensorflow:From /home/zongzan/.conda/envs/STR36/lib/python3.6/site-packages/tensorflow_core/python/ops/stropt/lms.py:970: The name tf.train.SessionRunHook is deprecated. Please use tf.estimator.SessionRunHook instead.

2022-01-06 12:48:38.645408: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.52GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2022-01-06 12:48:38.645473: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.52GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2022-01-06 12:48:38.699473: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.02GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2022-01-06 12:48:38.699518: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.02GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2022-01-06 12:48:38.699552: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 784.19MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2022-01-06 12:48:38.699571: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 784.19MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2022-01-06 12:48:38.708050: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 803.54MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2022-01-06 12:48:38.708083: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 803.54MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2022-01-06 12:48:38.713200: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 781.04MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2022-01-06 12:48:38.713235: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 781.04MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
1 Physical GPUs, 1 Logical GPUs
Parsing arguments: [('batch_size', 340), ('epochs', 1), ('model_name', 'MobileNet'), ('strategy', 'none'), ('verbose', False)]
Cannot find config for batch_size=340, use 512 instead
Using original tensorflow without memory optimization
Shape of x_train: (40000, 224, 224, 3), shape of x_test: (10000, 224, 224, 3)
[STR DEBUG] Parsing STR configuration: [{'strategy': 'none', 'verbose': 'false'}]
[STR DEBUG] Solution type none doesn't has corresponding optimizations, run normal training
  1/118 [..............................] - ETA: 28:53 - loss: 2.3370 - acc: 0.1206  2/118 [..............................] - ETA: 15:31 - loss: 3.0363 - acc: 0.1338  3/118 [..............................] - ETA: 10:57 - loss: 3.0024 - acc: 0.1265  4/118 [>.............................] - ETA: 8:41 - loss: 2.8849 - acc: 0.1360   5/118 [>.............................] - ETA: 7:18 - loss: 2.7987 - acc: 0.1382  6/118 [>.............................] - ETA: 6:22 - loss: 2.6984 - acc: 0.1549  7/118 [>.............................] - ETA: 5:43 - loss: 2.6558 - acc: 0.1517  8/118 [=>............................] - ETA: 5:13 - loss: 2.6203 - acc: 0.1544  9/118 [=>............................] - ETA: 4:50 - loss: 2.5654 - acc: 0.1627 10/118 [=>............................] - ETA: 4:32 - loss: 2.4992 - acc: 0.1729 11/118 [=>............................] - ETA: 4:16 - loss: 2.4545 - acc: 0.1810 12/118 [==>...........................] - ETA: 4:02 - loss: 2.4246 - acc: 0.1821 13/118 [==>...........................] - ETA: 3:51 - loss: 2.3887 - acc: 0.1905 14/118 [==>...........................] - ETA: 3:42 - loss: 2.3550 - acc: 0.1964 15/118 [==>...........................] - ETA: 3:33 - loss: 2.3238 - acc: 0.2016 16/118 [===>..........................] - ETA: 3:25 - loss: 2.2971 - acc: 0.2070 17/118 [===>..........................] - ETA: 3:18 - loss: 2.2718 - acc: 0.2130 18/118 [===>..........................] - ETA: 3:12 - loss: 2.2516 - acc: 0.2176 19/118 [===>..........................] - ETA: 3:06 - loss: 2.2318 - acc: 0.2224 20/118 [====>.........................] - ETA: 3:01 - loss: 2.2121 - acc: 0.2271 21/118 [====>.........................] - ETA: 2:56 - loss: 2.1949 - acc: 0.2314 22/118 [====>.........................] - ETA: 2:52 - loss: 2.1730 - acc: 0.2366 23/118 [====>.........................] - ETA: 2:48 - loss: 2.1574 - acc: 0.2419 24/118 [=====>........................] - ETA: 2:44 - loss: 2.1423 - acc: 0.2468 25/118 [=====>........................] - ETA: 2:41 - loss: 2.1311 - acc: 0.2493 26/118 [=====>........................] - ETA: 2:37 - loss: 2.1200 - acc: 0.2533 27/118 [=====>........................] - ETA: 2:34 - loss: 2.1043 - acc: 0.2574 28/118 [======>.......................] - ETA: 2:31 - loss: 2.0897 - acc: 0.2618 29/118 [======>.......................] - ETA: 2:28 - loss: 2.0789 - acc: 0.2647 30/118 [======>.......................] - ETA: 2:25 - loss: 2.0669 - acc: 0.26912022-01-06 12:49:25.814467: W tensorflow/core/common_runtime/bfc_allocator.cc:419] Allocator (GPU_0_bfc) ran out of memory trying to allocate 65.08MiB (rounded to 68239360).  Current allocation summary follows.
2022-01-06 12:49:25.817125: W tensorflow/core/common_runtime/bfc_allocator.cc:424] ****************************************************************************************************
2022-01-06 12:49:25.817186: W tensorflow/core/framework/op_kernel.cc:1651] OP_REQUIRES failed at transpose_op.cc:198 : Resource exhausted: OOM when allocating tensor with shape[340,1024,7,7] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
Traceback (most recent call last):
  File "../run_training.py", line 83, in <module>
    verbose=1)
  File "/home/zongzan/.conda/envs/STR36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py", line 728, in fit
    use_multiprocessing=use_multiprocessing)
  File "/home/zongzan/.conda/envs/STR36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_generator.py", line 603, in fit
    steps_name='steps_per_epoch')
  File "/home/zongzan/.conda/envs/STR36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_generator.py", line 265, in model_iteration
    batch_outs = batch_function(*batch_data)
  File "/home/zongzan/.conda/envs/STR36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py", line 1021, in train_on_batch
    outputs = self.train_function(ins)  # pylint: disable=not-callable
  File "/home/zongzan/.conda/envs/STR36/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py", line 3476, in __call__
    run_metadata=self.run_metadata)
  File "/home/zongzan/.conda/envs/STR36/lib/python3.6/site-packages/tensorflow_core/python/client/session.py", line 1472, in __call__
    run_metadata_ptr)
tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[340,1024,7,7] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
	 [[{{node training/RMSprop/gradients/gradients/conv_pw_13_relu/Relu6_grad/Relu6Grad-0-TransposeNHWCToNCHW-LayoutOptimizer}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.


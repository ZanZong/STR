WARNING:tensorflow:From ../run_training.py:27: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

WARNING:tensorflow:From ../run_training.py:27: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

WARNING:tensorflow:From ../run_training.py:27: The name tf.GPUOptions is deprecated. Please use tf.compat.v1.GPUOptions instead.

WARNING:tensorflow:From /home/zongzan/.conda/envs/STR36/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
1 Physical GPUs, 1 Logical GPUs
Parsing arguments: [('batch_size', 55), ('epochs', 1), ('model_name', 'vgg_unet'), ('strategy', 'none'), ('verbose', False)]
Cannot find config for batch_size=55, use 40 instead
Using original tensorflow without memory optimization
There are 367 and 367 images downloaded for train and test
Verifying train dataset
  0%|          | 0/367 [00:00<?, ?it/s]  2%|▏         | 8/367 [00:00<00:04, 74.73it/s]  4%|▍         | 16/367 [00:00<00:04, 74.28it/s]  7%|▋         | 27/367 [00:00<00:03, 89.54it/s] 11%|█         | 40/367 [00:00<00:03, 104.56it/s] 15%|█▍        | 54/367 [00:00<00:02, 114.27it/s] 18%|█▊        | 67/367 [00:00<00:02, 118.74it/s] 22%|██▏       | 80/367 [00:00<00:02, 121.81it/s] 25%|██▌       | 93/367 [00:00<00:02, 122.62it/s] 29%|██▉       | 107/367 [00:00<00:02, 125.69it/s] 33%|███▎      | 121/367 [00:01<00:01, 129.10it/s] 37%|███▋      | 137/367 [00:01<00:01, 137.66it/s] 42%|████▏     | 153/367 [00:01<00:01, 144.13it/s] 46%|████▋     | 170/367 [00:01<00:01, 149.76it/s] 51%|█████     | 187/367 [00:01<00:01, 153.52it/s] 55%|█████▌    | 203/367 [00:01<00:01, 154.42it/s] 60%|█████▉    | 220/367 [00:01<00:00, 156.60it/s] 64%|██████▍   | 236/367 [00:01<00:00, 157.38it/s] 69%|██████▊   | 252/367 [00:01<00:00, 157.48it/s] 73%|███████▎  | 269/367 [00:01<00:00, 158.49it/s] 78%|███████▊  | 286/367 [00:02<00:00, 159.03it/s] 82%|████████▏ | 302/367 [00:02<00:00, 159.18it/s] 87%|████████▋ | 319/367 [00:02<00:00, 160.06it/s] 92%|█████████▏| 336/367 [00:02<00:00, 160.12it/s] 96%|█████████▌| 353/367 [00:02<00:00, 159.90it/s]100%|██████████| 367/367 [00:02<00:00, 142.32it/s]
WARNING:tensorflow:
The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
  * https://github.com/tensorflow/io (for I/O related ops)
If you depend on functionality not listed there, please file an issue.

WARNING:tensorflow:From /home/zongzan/.conda/envs/STR36/lib/python3.6/site-packages/tensorflow_core/python/ops/stropt/lms.py:970: The name tf.train.SessionRunHook is deprecated. Please use tf.estimator.SessionRunHook instead.

WARNING:tensorflow:From /home/zongzan/.conda/envs/STR36/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
2022-01-05 02:32:51.395744: W tensorflow/core/common_runtime/bfc_allocator.cc:419] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.05GiB (rounded to 2203407360).  Current allocation summary follows.
2022-01-05 02:32:51.396729: W tensorflow/core/common_runtime/bfc_allocator.cc:424] *************************_*****_____***************________*****************************************
2022-01-05 02:32:51.396809: W tensorflow/core/framework/op_kernel.cc:1651] OP_REQUIRES failed at pad_op.cc:122 : Resource exhausted: OOM when allocating tensor with shape[10560,162,322] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
Dataset verified! 
Starting Epoch  0
[STR DEBUG] Parsing STR configuration: [{'strategy': 'none', 'verbose': 'false'}]
[STR DEBUG] Solution type none doesn't has corresponding optimizations, run normal training
Traceback (most recent call last):
  File "../run_training.py", line 68, in <module>
    epochs=1)
  File "/home/zongzan/.conda/envs/STR36/lib/python3.6/site-packages/keras_segmentation-0.2.0remat-py3.6.egg/keras_segmentation/train.py", line 106, in train
  File "/home/zongzan/.conda/envs/STR36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py", line 1301, in fit_generator
    steps_name='steps_per_epoch')
  File "/home/zongzan/.conda/envs/STR36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_generator.py", line 265, in model_iteration
    batch_outs = batch_function(*batch_data)
  File "/home/zongzan/.conda/envs/STR36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py", line 1021, in train_on_batch
    outputs = self.train_function(ins)  # pylint: disable=not-callable
  File "/home/zongzan/.conda/envs/STR36/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py", line 3476, in __call__
    run_metadata=self.run_metadata)
  File "/home/zongzan/.conda/envs/STR36/lib/python3.6/site-packages/tensorflow_core/python/client/session.py", line 1472, in __call__
    run_metadata_ptr)
tensorflow.python.framework.errors_impl.ResourceExhaustedError: 2 root error(s) found.
  (0) Resource exhausted: OOM when allocating tensor with shape[10560,162,322] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
	 [[{{node zero_padding2d_3/Pad}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

	 [[metrics/acc/Identity/_425]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

  (1) Resource exhausted: OOM when allocating tensor with shape[10560,162,322] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
	 [[{{node zero_padding2d_3/Pad}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

0 successful operations.
0 derived errors ignored.

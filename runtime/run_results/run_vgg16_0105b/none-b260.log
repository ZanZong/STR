WARNING:tensorflow:From ../run_training.py:27: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

WARNING:tensorflow:From ../run_training.py:27: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

WARNING:tensorflow:From ../run_training.py:27: The name tf.GPUOptions is deprecated. Please use tf.compat.v1.GPUOptions instead.

WARNING:tensorflow:From /home/zongzan/.conda/envs/STR36/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
WARNING:tensorflow:
The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
  * https://github.com/tensorflow/io (for I/O related ops)
If you depend on functionality not listed there, please file an issue.

WARNING:tensorflow:From /home/zongzan/.conda/envs/STR36/lib/python3.6/site-packages/tensorflow_core/python/ops/stropt/lms.py:970: The name tf.train.SessionRunHook is deprecated. Please use tf.estimator.SessionRunHook instead.

2022-01-05 15:03:15.702581: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.68GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2022-01-05 15:03:15.702644: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.68GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
1 Physical GPUs, 1 Logical GPUs
Parsing arguments: [('batch_size', 260), ('epochs', 1), ('model_name', 'VGG16'), ('strategy', 'none'), ('verbose', False)]
Cannot find config for batch_size=260, use 256 instead
Using original tensorflow without memory optimization
Shape of x_train: (40000, 224, 224, 3), shape of x_test: (10000, 224, 224, 3)
[STR DEBUG] Parsing STR configuration: [{'strategy': 'none', 'verbose': 'false'}]
[STR DEBUG] Solution type none doesn't has corresponding optimizations, run normal training
  1/154 [..............................] - ETA: 1:00:54 - loss: 2.3401 - acc: 0.11152022-01-05 15:03:22.256316: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.51GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
  2/154 [..............................] - ETA: 32:33 - loss: 12883540.1700 - acc: 0.10962022-01-05 15:03:23.997771: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.51GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
  3/154 [..............................] - ETA: 23:02 - loss: 8589049.1632 - acc: 0.1115 2022-01-05 15:03:25.780060: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.51GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
  4/154 [..............................] - ETA: 18:18 - loss: 6441788.4855 - acc: 0.10672022-01-05 15:03:27.581940: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.51GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
  5/154 [..............................] - ETA: 15:26 - loss: 5153445.8240 - acc: 0.10152022-01-05 15:03:29.444480: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.51GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
  6/154 [>.............................] - ETA: 13:32 - loss: 4294538.6523 - acc: 0.10322022-01-05 15:03:31.267816: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.51GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
  7/154 [>.............................] - ETA: 12:10 - loss: 3681033.5577 - acc: 0.10662022-01-05 15:03:33.082832: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.51GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2022-01-05 15:03:33.083616: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.51GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
  8/154 [>.............................] - ETA: 11:09 - loss: 3220905.5023 - acc: 0.1043  9/154 [>.............................] - ETA: 10:19 - loss: 2863027.3697 - acc: 0.1009 10/154 [>.............................] - ETA: 9:39 - loss: 2576724.8634 - acc: 0.1050  11/154 [=>............................] - ETA: 9:08 - loss: 2342477.3583 - acc: 0.1028 12/154 [=>............................] - ETA: 8:41 - loss: 2147271.1037 - acc: 0.1006 13/154 [=>............................] - ETA: 8:19 - loss: 1982096.5801 - acc: 0.1027 14/154 [=>............................] - ETA: 7:59 - loss: 1840518.4172 - acc: 0.1033 15/154 [=>............................] - ETA: 7:41 - loss: 1717817.3430 - acc: 0.1038 16/154 [==>...........................] - ETA: 7:26 - loss: 1610453.9029 - acc: 0.1031 17/154 [==>...........................] - ETA: 7:12 - loss: 1515721.4561 - acc: 0.1029 18/154 [==>...........................] - ETA: 7:00 - loss: 1431514.8365 - acc: 0.1013 19/154 [==>...........................] - ETA: 6:49 - loss: 1356172.0714 - acc: 0.1034 20/154 [==>...........................] - ETA: 6:40 - loss: 1288363.5830 - acc: 0.1044 21/154 [===>..........................] - ETA: 6:32 - loss: 1227013.0457 - acc: 0.1031 22/154 [===>..........................] - ETA: 6:24 - loss: 1171239.8328 - acc: 0.1033 23/154 [===>..........................] - ETA: 6:15 - loss: 1120316.4622 - acc: 0.1023 24/154 [===>..........................] - ETA: 6:07 - loss: 1073636.7050 - acc: 0.1030 25/154 [===>..........................] - ETA: 6:00 - loss: 1030691.3357 - acc: 0.1018 26/154 [====>.........................] - ETA: 5:53 - loss: 991049.4498 - acc: 0.1013  27/154 [====>.........................] - ETA: 5:47 - loss: 954344.0006 - acc: 0.1009 28/154 [====>.........................] - ETA: 5:41 - loss: 920260.3685 - acc: 0.1011 29/154 [====>.........................] - ETA: 5:34 - loss: 888527.3318 - acc: 0.0999 30/154 [====>.........................] - ETA: 5:29 - loss: 858909.8306 - acc: 0.1001 31/154 [=====>........................] - ETA: 5:25 - loss: 831203.1374 - acc: 0.1002 32/154 [=====>........................] - ETA: 5:21 - loss: 805228.1126 - acc: 0.0998 33/154 [=====>........................] - ETA: 5:16 - loss: 780827.3305 - acc: 0.0999 34/154 [=====>........................] - ETA: 5:11 - loss: 757861.8887 - acc: 0.0998 35/154 [=====>........................] - ETA: 5:06 - loss: 736208.7577 - acc: 0.1002 36/154 [======>.......................] - ETA: 5:02 - loss: 715758.5785 - acc: 0.0995 37/154 [======>.......................] - ETA: 4:58 - loss: 696413.8143 - acc: 0.0995 38/154 [======>.......................] - ETA: 4:53 - loss: 678087.1955 - acc: 0.1005 39/154 [======>.......................] - ETA: 4:49 - loss: 660700.4034 - acc: 0.1004 40/154 [======>.......................] - ETA: 4:45 - loss: 644182.9508 - acc: 0.1010 41/154 [======>.......................] - ETA: 4:42 - loss: 628471.2277 - acc: 0.1001 42/154 [=======>......................] - ETA: 4:38 - loss: 613507.6818 - acc: 0.1002 43/154 [=======>......................] - ETA: 4:35 - loss: 599240.1144 - acc: 0.1028 44/154 [=======>......................] - ETA: 4:31 - loss: 585621.1002 - acc: 0.1035 45/154 [=======>......................] - ETA: 4:27 - loss: 572607.3491 - acc: 0.1032 46/154 [=======>......................] - ETA: 4:24 - loss: 560159.4134 - acc: 0.1027 47/154 [========>.....................] - ETA: 4:20 - loss: 548241.1770 - acc: 0.1024 48/154 [========>.....................] - ETA: 4:17 - loss: 536819.5338 - acc: 0.1025 49/154 [========>.....................] - ETA: 4:14 - loss: 525864.0799 - acc: 0.1025 50/154 [========>.....................] - ETA: 4:11 - loss: 515346.8456 - acc: 0.1028 51/154 [========>.....................] - ETA: 4:07 - loss: 505242.0506 - acc: 0.1030 52/154 [=========>....................] - ETA: 4:04 - loss: 495525.9015 - acc: 0.1032 53/154 [=========>....................] - ETA: 4:01 - loss: 486176.3995 - acc: 0.1039 54/154 [=========>....................] - ETA: 3:58 - loss: 477173.1750 - acc: 0.1050 55/154 [=========>....................] - ETA: 3:55 - loss: 468497.3436 - acc: 0.1056 56/154 [=========>....................] - ETA: 3:51 - loss: 460131.3619 - acc: 0.1054 57/154 [==========>...................] - ETA: 3:48 - loss: 452058.9222 - acc: 0.1057 58/154 [==========>...................] - ETA: 3:46 - loss: 444264.8425 - acc: 0.1054 59/154 [==========>...................] - ETA: 3:43 - loss: 436734.9689 - acc: 0.1051 60/154 [==========>...................] - ETA: 3:40 - loss: 429456.0926 - acc: 0.1054 61/154 [==========>...................] - ETA: 3:37 - loss: 422415.8665 - acc: 0.1057 62/154 [===========>..................] - ETA: 3:34 - loss: 415602.7445 - acc: 0.1054 63/154 [===========>..................] - ETA: 3:31 - loss: 409005.9124 - acc: 0.1046 64/154 [===========>..................] - ETA: 3:28 - loss: 402615.2310 - acc: 0.1044 65/154 [===========>..................] - ETA: 3:25 - loss: 396421.1859 - acc: 0.1044 66/154 [===========>..................] - ETA: 3:22 - loss: 390414.8392 - acc: 0.1045 67/154 [============>.................] - ETA: 3:19 - loss: 384587.7864 - acc: 0.1045 68/154 [============>.................] - ETA: 3:16 - loss: 378932.1176 - acc: 0.1044 69/154 [============>.................] - ETA: 3:14 - loss: 373440.3811 - acc: 0.1047 70/154 [============>.................] - ETA: 3:11 - loss: 368105.5514 - acc: 0.1051 71/154 [============>.................] - ETA: 3:08 - loss: 362920.9986 - acc: 0.1054 72/154 [=============>................] - ETA: 3:06 - loss: 357880.4612 - acc: 0.1054 73/154 [=============>................] - ETA: 3:03 - loss: 352978.0206 - acc: 0.1055 74/154 [=============>................] - ETA: 3:00 - loss: 348208.0785 - acc: 0.1049 75/154 [=============>................] - ETA: 2:58 - loss: 343565.3348 - acc: 0.10512022-01-05 15:06:00.723507: W tensorflow/core/kernels/gpu_utils.cc:48] Failed to allocate memory for convolution redzone checking; skipping this check. This is benign and only means that we won't check cudnn for out-of-bounds reads and writes. This message will only be printed once.
 76/154 [=============>................] - ETA: 3:09 - loss: 339044.7685 - acc: 0.1052 77/154 [==============>...............] - ETA: 3:06 - loss: 334641.6197 - acc: 0.10552022-01-05 15:06:15.131915: W tensorflow/core/common_runtime/bfc_allocator.cc:419] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.11GiB (rounded to 3339714560).  Current allocation summary follows.
2022-01-05 15:06:15.132876: W tensorflow/core/common_runtime/bfc_allocator.cc:424] **************__*****************_*____________**_____*_____________***_***************___*___******
2022-01-05 15:06:15.132928: W tensorflow/core/framework/op_kernel.cc:1651] OP_REQUIRES failed at conv_grad_input_ops.cc:1063 : Resource exhausted: OOM when allocating tensor with shape[260,64,224,224] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
Traceback (most recent call last):
  File "../run_training.py", line 83, in <module>
    verbose=1)
  File "/home/zongzan/.conda/envs/STR36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py", line 728, in fit
    use_multiprocessing=use_multiprocessing)
  File "/home/zongzan/.conda/envs/STR36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_generator.py", line 603, in fit
    steps_name='steps_per_epoch')
  File "/home/zongzan/.conda/envs/STR36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_generator.py", line 265, in model_iteration
    batch_outs = batch_function(*batch_data)
  File "/home/zongzan/.conda/envs/STR36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py", line 1021, in train_on_batch
    outputs = self.train_function(ins)  # pylint: disable=not-callable
  File "/home/zongzan/.conda/envs/STR36/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py", line 3476, in __call__
    run_metadata=self.run_metadata)
  File "/home/zongzan/.conda/envs/STR36/lib/python3.6/site-packages/tensorflow_core/python/client/session.py", line 1472, in __call__
    run_metadata_ptr)
tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[260,64,224,224] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
	 [[{{node training/RMSprop/gradients/gradients/block1_conv2/Conv2D_grad/Conv2DBackpropInput}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.


WARNING:tensorflow:From ../run_training.py:27: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

WARNING:tensorflow:From ../run_training.py:27: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

WARNING:tensorflow:From ../run_training.py:27: The name tf.GPUOptions is deprecated. Please use tf.compat.v1.GPUOptions instead.

WARNING:tensorflow:From /home/zongzan/.conda/envs/STR36/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
WARNING:tensorflow:
The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
  * https://github.com/tensorflow/io (for I/O related ops)
If you depend on functionality not listed there, please file an issue.

WARNING:tensorflow:From /home/zongzan/.conda/envs/STR36/lib/python3.6/site-packages/tensorflow_core/python/ops/stropt/lms.py:970: The name tf.train.SessionRunHook is deprecated. Please use tf.estimator.SessionRunHook instead.

WARNING:tensorflow:From /home/zongzan/.conda/envs/STR36/lib/python3.6/site-packages/tensorflow_core/python/ops/stropt/lms.py:928: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.

WARNING:tensorflow:From /home/zongzan/.conda/envs/STR36/lib/python3.6/site-packages/tensorflow_core/contrib/graph_editor/select.py:554: get_backward_walk_ops (from tensorflow.contrib.graph_editor.select) is deprecated and will be removed after 2019-06-06.
Instructions for updating:
Please use tensorflow.python.ops.op_selector.get_backward_walk_ops.
2022-01-04 13:52:24.245952: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.01GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2022-01-04 13:52:24.246019: W tensorflow/core/kernels/gpu_utils.cc:48] Failed to allocate memory for convolution redzone checking; skipping this check. This is benign and only means that we won't check cudnn for out-of-bounds reads and writes. This message will only be printed once.
1 Physical GPUs, 1 Logical GPUs
Parsing arguments: [('batch_size', 250), ('epochs', 1), ('model_name', 'VGG16'), ('strategy', 'dynprog'), ('verbose', False)]
Shape of x_train: (50000, 224, 224, 3), shape of x_test: (10000, 224, 224, 3)
[STR DEBUG] Parsing STR configuration: [{'strategy': 'swap', 'verbose': 'true', 'tags': '/home/zongzan/dist_dnn_training/STR/optimizer/logs/VGG16_250/layer_names'}, {'p': '/home/zongzan/dist_dnn_training/STR/optimizer/logs/VGG16_250/P-dynprog', 'q': '/home/zongzan/dist_dnn_training/STR/optimizer/logs/VGG16_250/Q-dynprog'}]
[STR DEBUG] Use swap strategy of DYNPROG
[STR DEBUG] Processing layer 2's swapping, swap out at [3], swap in at [41]
[STR DEBUG] Find swapout ops: name: "block1_conv2/Relu"
op: "Relu"
input: "block1_conv2/BiasAdd"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
, 
	choose swap tensor Tensor("block1_conv2/Relu:0", shape=(?, 224, 224, 64), dtype=float32), 
	finish at the end of ops: [<tf.Operation 'block1_pool/MaxPool' type=MaxPool>]
[STR DEBUG] Add swap-in op at name: "training/RMSprop/gradients/gradients/block1_pool/MaxPool_grad/MaxPoolGrad"
op: "MaxPoolGrad"
input: "block1_conv2/Relu"
input: "block1_pool/MaxPool"
input: "training/RMSprop/gradients/gradients/block2_conv1/Conv2D_grad/Conv2DBackpropInput"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
attr {
  key: "data_format"
  value {
    s: "NHWC"
  }
}
attr {
  key: "ksize"
  value {
    list {
      i: 1
      i: 2
      i: 2
      i: 1
    }
  }
}
attr {
  key: "padding"
  value {
    s: "VALID"
  }
}
attr {
  key: "strides"
  value {
    list {
      i: 1
      i: 2
      i: 2
      i: 1
    }
  }
}
 for consumer [<tf.Operation 'training/RMSprop/gradients/gradients/block1_conv2/Relu_grad/ReluGrad' type=ReluGrad>]
[STR DEBUG] Processing layer 5's swapping, swap out at [6], swap in at [38]
[STR DEBUG] Find swapout ops: name: "block2_conv2/Relu"
op: "Relu"
input: "block2_conv2/BiasAdd"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
, 
	choose swap tensor Tensor("block2_conv2/Relu:0", shape=(?, 112, 112, 128), dtype=float32), 
	finish at the end of ops: [<tf.Operation 'block2_pool/MaxPool' type=MaxPool>]
[STR DEBUG] Add swap-in op at name: "training/RMSprop/gradients/gradients/block2_pool/MaxPool_grad/MaxPoolGrad"
op: "MaxPoolGrad"
input: "block2_conv2/Relu"
input: "block2_pool/MaxPool"
input: "training/RMSprop/gradients/gradients/block3_conv1/Conv2D_grad/Conv2DBackpropInput"
attr {
  key: "T"
  value {
    type: DT_FLOAT
  }
}
attr {
  key: "data_format"
  value {
    s: "NHWC"
  }
}
attr {
  key: "ksize"
  value {
    list {
      i: 1
      i: 2
      i: 2
      i: 1
    }
  }
}
attr {
  key: "padding"
  value {
    s: "VALID"
  }
}
attr {
  key: "strides"
  value {
    list {
      i: 1
      i: 2
      i: 2
      i: 1
    }
  }
}
 for consumer [<tf.Operation 'training/RMSprop/gradients/gradients/block2_conv2/Relu_grad/ReluGrad' type=ReluGrad>]
  1/200 [..............................] - ETA: 1:34:38 - loss: 2.3319 - acc: 0.09602022-01-04 13:52:26.345914: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.45GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2022-01-04 13:52:26.346268: W tensorflow/core/common_runtime/bfc_allocator.cc:239] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.45GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
  2/200 [..............................] - ETA: 50:58 - loss: 18054347.1660 - acc: 0.0920  3/200 [..............................] - ETA: 36:02 - loss: 12036240.3809 - acc: 0.0933  4/200 [..............................] - ETA: 28:44 - loss: 9028761.5689 - acc: 0.1000   5/200 [..............................] - ETA: 24:21 - loss: 7223012.3170 - acc: 0.0984  6/200 [..............................] - ETA: 21:24 - loss: 6019177.4608 - acc: 0.0980  7/200 [>.............................] - ETA: 19:13 - loss: 5159297.5852 - acc: 0.0966  8/200 [>.............................] - ETA: 17:44 - loss: 4514385.7229 - acc: 0.0940  9/200 [>.............................] - ETA: 16:32 - loss: 4012787.5673 - acc: 0.0969 10/200 [>.............................] - ETA: 15:33 - loss: 3611509.0412 - acc: 0.0972 11/200 [>.............................] - ETA: 14:44 - loss: 3283190.2482 - acc: 0.0949 12/200 [>.............................] - ETA: 14:03 - loss: 3009591.2527 - acc: 0.0980 13/200 [>.............................] - ETA: 13:32 - loss: 2778084.4106 - acc: 0.0957 14/200 [=>............................] - ETA: 13:06 - loss: 2579649.9743 - acc: 0.0977 15/200 [=>............................] - ETA: 12:40 - loss: 2407673.4629 - acc: 0.0971 16/200 [=>............................] - ETA: 12:17 - loss: 2257194.0153 - acc: 0.1000 17/200 [=>............................] - ETA: 11:57 - loss: 2124418.0320 - acc: 0.0993 18/200 [=>............................] - ETA: 11:40 - loss: 2006394.9361 - acc: 0.1000 19/200 [=>............................] - ETA: 11:25 - loss: 1900795.3236 - acc: 0.1004 20/200 [==>...........................] - ETA: 11:08 - loss: 1805755.6724 - acc: 0.1006 21/200 [==>...........................] - ETA: 10:54 - loss: 1719767.4164 - acc: 0.1006 22/200 [==>...........................] - ETA: 10:43 - loss: 1641596.2742 - acc: 0.1018 23/200 [==>...........................] - ETA: 10:31 - loss: 1570222.6226 - acc: 0.1042 24/200 [==>...........................] - ETA: 10:22 - loss: 1504796.8148 - acc: 0.1017 25/200 [==>...........................] - ETA: 10:13 - loss: 1444605.0337 - acc: 0.1030 26/200 [==>...........................] - ETA: 10:05 - loss: 1389043.3943 - acc: 0.1029 27/200 [===>..........................] - ETA: 9:57 - loss: 1337597.4276 - acc: 0.1046  28/200 [===>..........................] - ETA: 9:48 - loss: 1289826.1739 - acc: 0.1043 29/200 [===>..........................] - ETA: 9:42 - loss: 1245349.4887 - acc: 0.1051 30/200 [===>..........................] - ETA: 9:36 - loss: 1203837.9160 - acc: 0.1039 31/200 [===>..........................] - ETA: 9:30 - loss: 1165004.5090 - acc: 0.1044 32/200 [===>..........................] - ETA: 9:24 - loss: 1128598.1896 - acc: 0.1064 33/200 [===>..........................] - ETA: 9:18 - loss: 1094398.3775 - acc: 0.1064 34/200 [====>.........................] - ETA: 9:10 - loss: 1062210.2581 - acc: 0.1058 35/200 [====>.........................] - ETA: 9:04 - loss: 1031861.4593 - acc: 0.1051 36/200 [====>.........................] - ETA: 9:02 - loss: 1003198.7065 - acc: 0.1040 37/200 [====>.........................] - ETA: 8:57 - loss: 976085.2902 - acc: 0.1042  38/200 [====>.........................] - ETA: 8:51 - loss: 950398.8957 - acc: 0.1044 39/200 [====>.........................] - ETA: 8:45 - loss: 926029.7525 - acc: 0.1033 40/200 [=====>........................] - ETA: 8:40 - loss: 902879.0662 - acc: 0.1039 41/200 [=====>........................] - ETA: 8:34 - loss: 880857.6818 - acc: 0.1037 42/200 [=====>........................] - ETA: 8:29 - loss: 859884.9346 - acc: 0.1045 43/200 [=====>........................] - ETA: 8:25 - loss: 839887.6639 - acc: 0.1040 44/200 [=====>........................] - ETA: 8:21 - loss: 820799.3602 - acc: 0.1035 45/200 [=====>........................] - ETA: 8:16 - loss: 802559.4252 - acc: 0.1045 46/200 [=====>........................] - ETA: 8:12 - loss: 785112.5313 - acc: 0.1044 47/200 [======>.......................] - ETA: 8:08 - loss: 768408.0582 - acc: 0.1046 48/200 [======>.......................] - ETA: 8:04 - loss: 752399.6046 - acc: 0.1049 49/200 [======>.......................] - ETA: 8:00 - loss: 737044.5579 - acc: 0.1045 50/200 [======>.......................] - ETA: 7:56 - loss: 722303.7127 - acc: 0.1043 51/200 [======>.......................] - ETA: 7:52 - loss: 708140.9398 - acc: 0.1047 52/200 [======>.......................] - ETA: 7:47 - loss: 694522.8890 - acc: 0.1050 53/200 [======>.......................] - ETA: 7:44 - loss: 681418.7264 - acc: 0.10572022-01-04 13:54:56.791836: W tensorflow/core/common_runtime/bfc_allocator.cc:419] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.99GiB (rounded to 3211264000).  Current allocation summary follows.
2022-01-04 13:54:56.792330: W tensorflow/core/common_runtime/bfc_allocator.cc:424] ___________***_*______________**__**************_______**************_*****************_*_**__****_*
2022-01-04 13:54:56.792369: W tensorflow/core/framework/op_kernel.cc:1651] OP_REQUIRES failed at conv_grad_input_ops.cc:1063 : Resource exhausted: OOM when allocating tensor with shape[250,64,224,224] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
Traceback (most recent call last):
  File "../run_training.py", line 70, in <module>
    verbose=1)
  File "/home/zongzan/.conda/envs/STR36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py", line 728, in fit
    use_multiprocessing=use_multiprocessing)
  File "/home/zongzan/.conda/envs/STR36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_generator.py", line 603, in fit
    steps_name='steps_per_epoch')
  File "/home/zongzan/.conda/envs/STR36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_generator.py", line 265, in model_iteration
    batch_outs = batch_function(*batch_data)
  File "/home/zongzan/.conda/envs/STR36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py", line 1021, in train_on_batch
    outputs = self.train_function(ins)  # pylint: disable=not-callable
  File "/home/zongzan/.conda/envs/STR36/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py", line 3476, in __call__
    run_metadata=self.run_metadata)
  File "/home/zongzan/.conda/envs/STR36/lib/python3.6/site-packages/tensorflow_core/python/client/session.py", line 1472, in __call__
    run_metadata_ptr)
tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[250,64,224,224] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
	 [[{{node training/RMSprop/gradients/gradients/block1_conv2/Conv2D_grad/Conv2DBackpropInput}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.


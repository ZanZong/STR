import os
import re
import sys
import json
from collections import defaultdict
import numpy as np

def analysis_performance(path):
    """ Extract throughput from TF keras log
    """

    dir = os.listdir(path)
    logs = [fname for fname in dir if str(fname).__contains__(".log")]
    logs = sorted(logs, key=lambda name: int(re.findall(r"\d+", name)[-1]))
    for log in logs:
        log_name = os.path.join(path, log)
        print("Extract log: {}".format(log_name))
        with open(log_name, "r") as f:
            line = f.readline()
            while line:
                if line.__contains__("====") and line.__contains__("step"):
                    num_batches = int(re.findall(r"\d+/\d+", line)[0].split("/")[0])
                    batch_size = int(re.findall(r"\d+", log_name)[-1])
                    epoch_time = int(re.findall(r"\d+s ", line)[0][:-2])
                    throughput = 1 / (epoch_time / num_batches) * batch_size
                    print("Batch num: {}, batch size: {}, total time: {}s, throughput: {} img/s\n"
                                                .format(num_batches, batch_size, epoch_time, throughput))
                    break
                line = f.readline()

def parse_profile_timeline(path, layer_file):
    """Parsing json file generated by TF timeline.

    Args:
        path (str): timeline path
        layer_names (str): layer name path
    """
    layer_names = list()
    with open(layer_file, "r") as flayer:
        line = flayer.readline()
        while line:
            layer_names.append(line.strip())
            line = flayer.readline()

    costs = defaultdict(list)
    timelines = json.load(open(path, 'r'))
    timelines = timelines['traceEvents']
    is_bw = False
    pass_last_node = False
    loss_duration = False
    for tl in timelines:
        if "dur" not in tl.keys():
            continue
        name_index = str(tl["args"]["name"]).find("/")
        if name_index != -1:
            name = tl["args"]["name"][:name_index]
            if name == layer_names[-1]:
                pass_last_node = True
            if pass_last_node and name == "loss":
                loss_duration = True
            if loss_duration and name != "loss":
                is_bw = True
                loss_duration = False
            if is_bw:
                costs["bw_" + name].append(tl["dur"])
            else:
                costs[name].append(tl["dur"])
        else:
            if str(tl["args"]["name"]).__contains__(layer_names[0]):
                if is_bw:
                    if str(tl["args"]["name"]).__contains__("MemcpyHtoD"):
                        costs[layer_names[0]].append(tl["dur"])
                    else:
                        costs["bw_" + layer_names[0]].append(tl["dur"])
                else:
                    costs[layer_names[0]].append(tl["dur"])
    if "loss" in costs.keys() and layer_names[-1] in costs.keys():
        costs[layer_names[-1]].extend(costs["loss"])
    
    npcosts = np.zeros(len(layer_names) * 2 - 1, dtype=float)
    # fill forward
    for idx, name in enumerate(layer_names):
        if name in costs.keys():
            npcosts[idx] = sum(costs[name])
        else:
            print(f"Cannot find op='{name}' compute cost")
    back_index = len(layer_names)
    for idx, name in enumerate(layer_names[::-1][1:]):
        if "bw_" + name in costs.keys():
            npcosts[back_index + idx] = sum(costs["bw_" + name])
        else:
            print(f"Cannot find op='bw_{name}' compute cost")
    np.save("ResNet101_64_p32xlarge.npy", npcosts)



if __name__ == "__main__":
    # 1. Print throughput for logs in specific path
    log = sys.argv[1]
    analysis_performance(log)

    # 2. Write out npy file of profile logs
    # log = sys.argv[1]
    # layer_file = sys.argv[2]
    # log = "/home/zongzan/dist_dnn_training/STR/runtime/logs/timeline-resnet101-64.json"
    # layer_file = "/home/zongzan/dist_dnn_training/STR/optimizer/logs/ResNet101_64/layer_names"
    # parse_profile_timeline(log, layer_file)

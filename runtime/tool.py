import os
import re
import sys
import json
from collections import defaultdict
import numpy as np
import re

def analysis_performance(path):
    """ Extract throughput from TF keras log
    """

    dir = os.listdir(path)
    logs = [fname for fname in dir if str(fname).__contains__(".log")]
    logs = sorted(logs, key=lambda name: int(re.findall(r"\d+", name)[-1]))
    for log in logs:
        log_name = os.path.join(path, log)
        print("Extract log: {}".format(log_name))
        with open(log_name, "r") as f:
            line = f.readline()
            while line:
                if line.__contains__("====") and line.__contains__("step"):
                    num_batches = int(re.findall(r"\d+/\d+", line)[0].split("/")[0])
                    batch_size = int(re.findall(r"\d+", log_name)[-1])
                    epoch_time = int(re.findall(r"\d+s ", line)[0][:-2])
                    throughput = 1 / (epoch_time / num_batches) * batch_size
                    print("Batch num: {}, batch size: {}, total time: {}s, throughput: {} img/s\n"
                                                .format(num_batches, batch_size, epoch_time, throughput))
                    break
                line = f.readline()

def parse_profile_timeline(path, layer_file):
    """Parsing json file generated by TF timeline.

    Args:
        path (str): timeline path
        layer_names (str): layer name path
    """
    layer_names = list()
    with open(layer_file, "r") as flayer:
        line = flayer.readline()
        while line:
            layer_names.append(line.strip())
            line = flayer.readline()

    costs = defaultdict(list)
    timelines = json.load(open(path, 'r'))
    timelines = timelines['traceEvents']
    is_bw = False
    pass_last_node = False
    loss_duration = False
    for tl in timelines:
        if "dur" not in tl.keys():
            continue
        name_index = str(tl["args"]["name"]).find("/")
        if name_index != -1:
            name = tl["args"]["name"][:name_index]
            if name == layer_names[-1]:
                pass_last_node = True
            if pass_last_node and name == "loss":
                loss_duration = True
            if loss_duration and name != "loss":
                is_bw = True
                loss_duration = False
            if is_bw:
                costs["bw_" + name].append(tl["dur"])
            else:
                costs[name].append(tl["dur"])
        else:
            if str(tl["args"]["name"]).__contains__(layer_names[0]):
                if is_bw:
                    if str(tl["args"]["name"]).__contains__("MemcpyHtoD"):
                        costs[layer_names[0]].append(tl["dur"])
                    else:
                        costs["bw_" + layer_names[0]].append(tl["dur"])
                else:
                    costs[layer_names[0]].append(tl["dur"])
    if "loss" in costs.keys() and layer_names[-1] in costs.keys():
        costs[layer_names[-1]].extend(costs["loss"])
    
    npcosts = np.zeros(len(layer_names) * 2 - 1, dtype=float)
    # fill forward
    for idx, name in enumerate(layer_names):
        if name in costs.keys():
            npcosts[idx] = sum(costs[name])
        else:
            print(f"Cannot find op='{name}' compute cost")
    back_index = len(layer_names)
    for idx, name in enumerate(layer_names[::-1][1:]):
        if "bw_" + name in costs.keys():
            npcosts[back_index + idx] = sum(costs["bw_" + name])
        else:
            print(f"Cannot find op='bw_{name}' compute cost")
    np.save("ResNet101_64_p32xlarge.npy", npcosts)

def parse_transformer_timeline(path, deps_path, mems_path, batch_size):
    """Parsing json file generated by TF timeline for Transformer,
       which uses the second-level names, i.e., transformer/layer_normalization_2

    Args:
        path (str): timeline path
        deps_path (str): file path for node dependency
    """

    def get_shortname(name):
        splitter = [m.start() for m in re.finditer("/", name)]
        if len(splitter) >= 3:
            short_name = name[:splitter[2]] # Up to 3rd level op name
        else:
            short_name = name
        return short_name
            
    deps = json.load(open(deps_path, 'r'))
    mems = json.load(open(mems_path, 'r'))
    layer_names = list()
    short_deps = dict()
    short_mems = dict()

    for node_name in deps.keys():
        if str(node_name).startswith("loss") or str(node_name).startswith("metrics") \
            or str(node_name).startswith("Equal") or node_name.__contains__("ReadVariableOp") \
                or node_name.__contains__("/shape") or node_name.__contains__("/Shape") \
                    or node_name.__contains__("/unstack") or node_name.__contains__("weights_values") \
                        or node_name.__contains__("weights_keys") or node_name.__contains__("weights_queries") \
                            or node_name.__contains__("Cast") or node_name.__contains__("Equal") \
                                or node_name.__contains__("random_uniform") or node_name.__contains__("truediv"):
            continue
        layer_names.append(get_shortname(node_name))
        # float number in bytes
        if get_shortname(node_name) not in short_mems.keys():
            short_mems[get_shortname(node_name)] = 0
        short_mems[get_shortname(node_name)] += sum(mems[node_name]) * 32 * 4 if node_name in mems.keys() else 0
        if node_name in deps.keys():
            short_deps[get_shortname(node_name)] = [get_shortname(d) for d in deps[node_name] if get_shortname(d) in layer_names]
        else:
            short_deps[get_shortname(node_name)] = []
            
        if node_name == "dense/Softmax":
            break
    layer_names = sorted(set(layer_names), key=layer_names.index)
    
    costs = defaultdict(list)
    timelines = json.load(open(path, 'r'))
    timelines = timelines['traceEvents']
    profiled_op_names = list()
    for tl in timelines:
        if "dur" in tl.keys() and "args" in tl.keys() and "name" in tl["args"].keys():
            profiled_op_names.append(tl["args"]["name"])
    layer_names = [name for name in layer_names if name in profiled_op_names]
    for name in layer_names:
        if len(short_deps[name]) == 0 and name not in short_deps.values():
            del short_deps[name]
            layer_names.remove(name)
    
    print(f"Write out total nodes={len(layer_names)} to layer_names_transformer")
    with open(f"layer_names_transformer", "w") as f:
        for layer in layer_names:
            f.write(layer + "\n")
    print(f"Write out shorted node deps to transformer_layer_deps_xx.json")
    json.dump(short_deps, open(f"transformer-layer-deps-{batch_size}.json", "w"))

    # Parsing computing cost
    for tl in timelines:
        if "dur" not in tl.keys():
            continue
        for node_name in layer_names:
            if str(tl["args"]["name"]).startswith(node_name):
                cur_node_name = get_shortname(tl["args"]["name"])
                if cur_node_name == node_name:
                    costs[node_name].append(tl["dur"])
                    break
            elif str(tl["args"]["name"]).startswith("training"):
                if str(tl["args"]["name"]).__contains__(node_name+"_grad") or \
                    (str(tl["args"]["name"]).__contains__(node_name) and  not str(tl["args"]["name"]).__contains__("gradient")):
                        costs["bw_" + node_name].append(tl["dur"])
                        break

        if str(tl["args"]["name"]).startswith("loss") or str(tl["args"]["name"]).startswith("metrics"):
            costs[layer_names[-1]].append(tl["dur"])
    
    npcosts = np.zeros(len(layer_names) * 2 - 1, dtype=float)
    # fill forward
    for idx, name in enumerate(layer_names):
        if name in costs.keys():
            npcosts[idx] = sum(costs[name])
        else:
            print(f"Cannot find op='{name}' compute cost")
    back_index = len(layer_names)
    for idx, name in enumerate(layer_names[::-1][1:]):
        if "bw_" + name in costs.keys():
            npcosts[back_index + idx] = sum(costs["bw_" + name])
        else:
            # Some nodes don't have backward pass, i.e., add
            print(f"Cannot find op='bw_{name}' compute cost, delete corresponding forward")
            # npcosts[back_index - idx - 1] = 0
            # del_name.append(name)
            # # Update file
            # with open(f"layer_names_transformer", "w") as f:
            #     for layer in layer_names:
            #         f.write(layer + "\n")
    print(f"Write out profiling costs to transformer_xx_p32xlarge.npy")
    np.save(f"transformer_{batch_size}_p32xlarge.npy", npcosts)
    np_mem = np.zeros(len(layer_names), dtype=float)
    # float number in bytes
    for idx, name in enumerate(layer_names):
        np_mem[idx] = short_mems[name]
    print(np_mem)
    np.save(f"transformer_mem_{batch_size}.npy", np_mem)

if __name__ == "__main__":
    # 1. Print throughput for logs in specific path
    log = sys.argv[1]
    analysis_performance(log)

    # 2. Write out npy file of profile logs
    # log = "/home/zongzan/dist_dnn_training/STR/runtime/logs/timeline-resnet101-64.json"
    # layer_file = "/home/zongzan/dist_dnn_training/STR/optimizer/logs/ResNet101_64/layer_names"
    # parse_profile_timeline(log, layer_file)

    # 4. Write out npy file of profile logs for transformer
    # log = "/home/zongzan/dist_dnn_training/STR/runtime/logs/timeline-transformer-16.json"
    # # the following two logs from instrumentated tensorflow runtime
    # layer_file = "/home/zongzan/dist_dnn_training/STR/runtime/logs/transformer-deps-origin.json"
    # mem_file = "/home/zongzan/dist_dnn_training/STR/runtime/logs/transformer-16-mem.json"
    # parse_transformer_timeline(log, layer_file, mem_file, 16)
